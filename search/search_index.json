{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Autolab Docs Autolab is a course management platform that enables instructors to offer autograded programming assignments to their students. The two key ideas in Autolab are autograding that is, programs evaluating other programs, and scoreboards that display the latest autograded scores for each student. Autolab also provides gradebooks, rosters, handins/handouts, lab writeups, code annotation, manual grading, late penalties, grace days, cheat checking, meetings, partners, and bulk emails. For information on how to use Autolab for your course see the Guide for Instructors . To learn how to write an autograded lab see the Guide for Lab Authors . To get straight to an installation, go to Getting Started Demonstration Site If you would like to check out Autolab prior to installation, go over to our Demo Site ! Login through Developer Login with the email: admin@foo.bar . This is a demonstration website. It refreshes at 0,6,12,18 Hours (UTC) daily, and it is publicly accessible, so please only use it for your exploration. Do not use this site to store important information. Try the following in order: Create a new course Click on Manage Autolab (top-right navigation bar) > Create New Course . Fill in the name and semester, and then create to see your course on the homepage. (NOTE: the email doesn't need to be real here) Create an Autograded Lab Assessment. Go into the course you have just created, click on Install Assessment . You can install a simple autograded lab, called hello lab. Download hello.tar and install it using the Import from Tarball option. In the hello lab, students are asked to write a file called hello.c . The autograder checks that the submitted hello.c program compiles and runs with an exit status of zero. If so, the submission gets 100 points. Otherwise it gets 0 points. Try submitting to the autograded hello lab Create and submit a hello.c file. //hello.c #include <stdio.h> int main () { printf ( \"Hello, World!\" ); return 0 ; } Refresh the submitted entries page to see the autograded score appear Click on a sub score, in this case the 100.0 under the Correctness heading, to see the output from the autograder. For more information on hello lab, or how to create your own lab, go to Guide for Lab Authors ! Create a PDF homework assessment Autolab can also handle pdf submissions as well! Click on Install Assessment , then on Assessment Builder . Name your assessment, and give it a category and click Create Assessment !. Because it defaults to accepting .c files, we would like to change it to *.pdf . Click on Edit Assessment > Handin and then change the Handin filename to handin.pdf instead of handin.c and save the changes Try submitting to the pdf homework asssessment. Submit a .pdf file. Look at your submission using the magnifying glass icon Grading submissions Click on Grade Submissions , and then the arrow button to open up student submissions. For details on the relevant features for an Instructor, go to Guide for Instructor Getting Started Autolab consists of two services: (1) the Ruby on Rails frontend, and (2) Tango , the RESTful Python autograding server. Either service can run independently without the other. But in order to use all features of Autolab, we highly recommend installing both services. Currently, we have support for installing Autolab on Ubuntu 18.04+ , and Mac OSX . Mac OSX 10.11+ Follow the step-by-step instructions below: Install rbenv (use the Basic GitHub Checkout method) Install ruby-build as an rbenv plugin: git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build Restart your shell at this point in order to start using your newly installed rbenv Clone the Autolab repo into home directory and enter it: cd ~/ git clone https://github.com/autolab/Autolab.git && cd Autolab Install the correct version of ruby: rbenv install $( cat .ruby-version ) At this point, confirm that rbenv is working (you might need to restart your shell): $ which ruby ~/.rbenv/shims/ruby $ which rake ~/.rbenv/shims/rake Note that Mac OSX comes with its own installation of ruby. You might need to switch your ruby from the system version to the rbenv installed version. One option is to add the following lines to ~/.bash_profile: export RBENV_ROOT = <rbenv folder path on your local machine> eval \" $( rbenv init - ) \" Install bundler : gem install bundler rbenv rehash Install the required gems (run the following commands in the cloned Autolab repo): cd bin bundle install Refer to the FAQ for issues installing gems Install one of two database options SQLite should only be used in development MySQL can be used in development or production Configure your database: cp config/database.yml.template config/database.yml Edit database.yml with the correct credentials for your chosen database. Refer to the FAQ for any issues. Configure school/organization specific information (new feature): cp config/school.yml.template config/school.yml Edit school.yml with your school/organization specific names and emails Configure the Devise Auth System with a unique key (run these commands exactly - leave <YOUR-SECRET-KEY> as it is): cp config/initializers/devise.rb.template config/initializers/devise.rb sed -i \"s/<YOUR-SECRET-KEY>/`bundle exec rake secret`/g\" config/initializers/devise.rb Fill in <YOUR_WEBSITE> in the config/initializers/devise.rb file. To skip this step for now, fill with foo.bar . Create and initialize the database tables: bundle exec rake db:create bundle exec rake db:migrate Do not forget to use bundle exec in front of every rake/rails command. Populate dummy data (development only): bundle exec rake autolab:populate Start the rails server: bundle exec rails s -p 3000 Go to localhost:3000 and login with Developer Login : Email: \"admin@foo.bar\" . Install Tango , the backend autograding service. Now you are all set to start using Autolab! Visit the Guide for Instructors and Guide for Lab Authors pages for more info. Ubuntu 18.04+ This set of instruction is meant to install of AutoLab v2.40 on Ubuntu 18.04 LTS. Upgrade system packages and installing prerequisites sudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential git libffi-dev zlib1g-dev autoconf bison build-essential libssl-dev libyaml-dev libreadline6-dev libncurses5-dev libgdbm5 libgdbm-dev libmysqlclient-dev libjansson-dev ctags Cloning Autolab repo from Github to ~/Autolab cd ~/ git clone https://github.com/autolab/Autolab.git cd Autolab Setting up rbenv and ruby-build plugin cd ~/ git clone https://github.com/rbenv/rbenv.git ~/.rbenv echo 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' >> ~/.bashrc echo 'eval \"$(rbenv init -)\"' >> ~/.bashrc source ~/.bashrc ~/.rbenv/bin/rbenv init git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build Installing Ruby (Based on ruby version) cd Autolab rbenv install ` cat .ruby-version ` Installing SQLite sudo apt-get install sqlite3 libsqlite3-dev Installing MySQL. (If you would just like to test Autolab, then you can skip this step by using SQLite) Following instructions from How to Install MySQL on Ubuntu . sudo apt install mysql-server sudo mysql_secure_installation > There will be a few questions asked during the MySQL setup. * Validate Password Plugin? N * Remove Annonymous Users? Y * Disallow Root Login Remotely? Y * Remove Test Database and Access to it? Y * Reload Privilege Tables Now? Y (If you are using MySQL) Create a new user with access to autolab_test and autolab_development databases. Because a password rather than auth_socket is needed, we need to ensure that user uses mysql_native_password sudo mysql mysql> CREATE USER 'user1' @ 'localhost' IDENTIFIED WITH mysql_native_password BY '<password>' ; mysql> FLUSH PRIVILEGES ; mysql> exit ; Installing Rails cd Autolab gem install bundler rbenv rehash bundle install Initializing Autolab Configs cd Autolab cp config/database.yml.template config/database.yml cp config/school.yml.template config/school.yml cp config/initializers/devise.rb.template config/initializers/devise.rb sed -i \"s/<YOUR-SECRET-KEY>/`bundle exec rake secret`/g\" config/initializers/devise.rb cp config/autogradeConfig.rb.template config/autogradeConfig.rb (Using MySQL) Editing Database YML. Change the and fields in config/database.yml to the username and password that has been set up for the mysql. For example if your username is user1 , and your password is 123456 , then your yml would be development : adapter : mysql2 database : autolab_development pool : 5 username : user1 password : '123456' socket : /var/run/mysqld/ mysqld . sock host : localhost variables : sql_mode : NO_ENGINE_SUBSTITUTION test : adapter : mysql2 database : autolab_test pool : 5 username : user1 password : '123456' socket : /var/run/mysqld/ mysqld . sock host : localhost variables : sql_mode : NO_ENGINE_SUBSTITUTION (Using SQLite) Editing Database YML. Comment out the configurations meant for MySQL in config/database.yml, and insert the following development : adapter : sqlite3 database : db / autolab_development pool : 5 timeout : 5000 test : adapter : sqlite3 database : db / autolab_test pool : 5 timeout : 5000 Granting permissions on the databases. Setting global sql mode is important to relax the rules of mysql when it comes to group by mode ( access mysql using your root first to grant permissions ) mysql> grant all privileges on autolab_development.* to '<username>' @localhost ; mysql> grant all privileges on autolab_test.* to '<username>' @localhost ; mysql> SET GLOBAL sql_mode =( SELECT REPLACE ( @@sql_mode, 'ONLY_FULL_GROUP_BY' , '' )) ; mysql> exit Initializing Autolab Database cd Autolab bundle exec rake db:create bundle exec rake db:reset bundle exec rake db:migrate Populating sample course & students cd Autolab bundle exec rake autolab:populate Run Autolab! cd Autolab bundle exec rails s -p 3000 --binding = 0 .0.0.0 Visit localhost:3000 on your browser to view your local deployment of Autolab, and login with Developer Login Email: \"admin@foo.bar\" Install Tango , the backend autograding service. If you would like to deploy the server, you can try out Phusion Passenger Now you are all set to start using Autolab! Visit the Guide for Instructors and Guide for Lab Authors pages for more info. FAQ This is a general list of questions that we get often. If you find a solution to an issue not mentioned here, please contact us at autolab-dev@andrew.cmu.edu Ubuntu Script Bugs If you get the following error Failed to fetch http://dl.google.com/linux/chrome/deb/dists/stable/Release Unable to find expected entry 'main/binary-i386/Packages' in Release file ( Wrong sources.list entry or malformed file ) then follow the solution in this post . Where do I find the MySQL username and password? If this is your first time logging into MySQL, your username is 'root'. You may also need to set the root password: Start the server: sudo /usr/local/mysql/support-files/mysql.server start Set the password: mysqladmin -u root password \"[New_Password]\" If you lost your root password, refer to the MySQL wiki Bundle Install Errors This happens as gems get updated. These fixes are gem-specific, but two common ones are eventmachine bundle config build.eventmachine --with-cppflags = -I/usr/local/opt/openssl/include libv8 bundle config build.libv8 --with-system-v8 Run bundle install again If this does not work, another option would be bundle update libv8 Because updating libv8 has dependency on other gems, it might fail due to a need to update other gems. Just do bundle update <gem> according to the error messages until all gems are up to date. Run bundle install again If neither of these works, try exploring this StackOverflow link Can't connect to local MySQL server through socket Make sure you've started the MySQL server and double-check the socket in config/database.yml The default socket location is /tmp/mysql.sock . I forgot my MySQL root password You can reset it following the instructions on this Stack Overflow post If mysql complains that the password is expired, follow the instructions on the second answer on this post MySQL Syntax Error If you get the following error Mysql2::Error: You have an error in your SQL syntax this may be an issue with using an incompatible version of MySQL. Try switching to MySQL 5.7 if you are currently using a different version.","title":"Getting Started"},{"location":"#welcome-to-the-autolab-docs","text":"Autolab is a course management platform that enables instructors to offer autograded programming assignments to their students. The two key ideas in Autolab are autograding that is, programs evaluating other programs, and scoreboards that display the latest autograded scores for each student. Autolab also provides gradebooks, rosters, handins/handouts, lab writeups, code annotation, manual grading, late penalties, grace days, cheat checking, meetings, partners, and bulk emails. For information on how to use Autolab for your course see the Guide for Instructors . To learn how to write an autograded lab see the Guide for Lab Authors . To get straight to an installation, go to Getting Started","title":"Welcome to the Autolab Docs"},{"location":"#demonstration-site","text":"If you would like to check out Autolab prior to installation, go over to our Demo Site ! Login through Developer Login with the email: admin@foo.bar . This is a demonstration website. It refreshes at 0,6,12,18 Hours (UTC) daily, and it is publicly accessible, so please only use it for your exploration. Do not use this site to store important information. Try the following in order:","title":"Demonstration Site"},{"location":"#create-a-new-course","text":"Click on Manage Autolab (top-right navigation bar) > Create New Course . Fill in the name and semester, and then create to see your course on the homepage. (NOTE: the email doesn't need to be real here)","title":"Create a new course"},{"location":"#create-an-autograded-lab-assessment","text":"Go into the course you have just created, click on Install Assessment . You can install a simple autograded lab, called hello lab. Download hello.tar and install it using the Import from Tarball option. In the hello lab, students are asked to write a file called hello.c . The autograder checks that the submitted hello.c program compiles and runs with an exit status of zero. If so, the submission gets 100 points. Otherwise it gets 0 points. Try submitting to the autograded hello lab Create and submit a hello.c file. //hello.c #include <stdio.h> int main () { printf ( \"Hello, World!\" ); return 0 ; } Refresh the submitted entries page to see the autograded score appear Click on a sub score, in this case the 100.0 under the Correctness heading, to see the output from the autograder. For more information on hello lab, or how to create your own lab, go to Guide for Lab Authors !","title":"Create an Autograded Lab Assessment."},{"location":"#create-a-pdf-homework-assessment","text":"Autolab can also handle pdf submissions as well! Click on Install Assessment , then on Assessment Builder . Name your assessment, and give it a category and click Create Assessment !. Because it defaults to accepting .c files, we would like to change it to *.pdf . Click on Edit Assessment > Handin and then change the Handin filename to handin.pdf instead of handin.c and save the changes Try submitting to the pdf homework asssessment. Submit a .pdf file. Look at your submission using the magnifying glass icon","title":"Create a PDF homework assessment"},{"location":"#grading-submissions","text":"Click on Grade Submissions , and then the arrow button to open up student submissions. For details on the relevant features for an Instructor, go to Guide for Instructor","title":"Grading submissions"},{"location":"#getting-started","text":"Autolab consists of two services: (1) the Ruby on Rails frontend, and (2) Tango , the RESTful Python autograding server. Either service can run independently without the other. But in order to use all features of Autolab, we highly recommend installing both services. Currently, we have support for installing Autolab on Ubuntu 18.04+ , and Mac OSX .","title":"Getting Started"},{"location":"#mac-osx-1011","text":"Follow the step-by-step instructions below: Install rbenv (use the Basic GitHub Checkout method) Install ruby-build as an rbenv plugin: git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build Restart your shell at this point in order to start using your newly installed rbenv Clone the Autolab repo into home directory and enter it: cd ~/ git clone https://github.com/autolab/Autolab.git && cd Autolab Install the correct version of ruby: rbenv install $( cat .ruby-version ) At this point, confirm that rbenv is working (you might need to restart your shell): $ which ruby ~/.rbenv/shims/ruby $ which rake ~/.rbenv/shims/rake Note that Mac OSX comes with its own installation of ruby. You might need to switch your ruby from the system version to the rbenv installed version. One option is to add the following lines to ~/.bash_profile: export RBENV_ROOT = <rbenv folder path on your local machine> eval \" $( rbenv init - ) \" Install bundler : gem install bundler rbenv rehash Install the required gems (run the following commands in the cloned Autolab repo): cd bin bundle install Refer to the FAQ for issues installing gems Install one of two database options SQLite should only be used in development MySQL can be used in development or production Configure your database: cp config/database.yml.template config/database.yml Edit database.yml with the correct credentials for your chosen database. Refer to the FAQ for any issues. Configure school/organization specific information (new feature): cp config/school.yml.template config/school.yml Edit school.yml with your school/organization specific names and emails Configure the Devise Auth System with a unique key (run these commands exactly - leave <YOUR-SECRET-KEY> as it is): cp config/initializers/devise.rb.template config/initializers/devise.rb sed -i \"s/<YOUR-SECRET-KEY>/`bundle exec rake secret`/g\" config/initializers/devise.rb Fill in <YOUR_WEBSITE> in the config/initializers/devise.rb file. To skip this step for now, fill with foo.bar . Create and initialize the database tables: bundle exec rake db:create bundle exec rake db:migrate Do not forget to use bundle exec in front of every rake/rails command. Populate dummy data (development only): bundle exec rake autolab:populate Start the rails server: bundle exec rails s -p 3000 Go to localhost:3000 and login with Developer Login : Email: \"admin@foo.bar\" . Install Tango , the backend autograding service. Now you are all set to start using Autolab! Visit the Guide for Instructors and Guide for Lab Authors pages for more info.","title":"Mac OSX 10.11+"},{"location":"#ubuntu-1804","text":"This set of instruction is meant to install of AutoLab v2.40 on Ubuntu 18.04 LTS. Upgrade system packages and installing prerequisites sudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential git libffi-dev zlib1g-dev autoconf bison build-essential libssl-dev libyaml-dev libreadline6-dev libncurses5-dev libgdbm5 libgdbm-dev libmysqlclient-dev libjansson-dev ctags Cloning Autolab repo from Github to ~/Autolab cd ~/ git clone https://github.com/autolab/Autolab.git cd Autolab Setting up rbenv and ruby-build plugin cd ~/ git clone https://github.com/rbenv/rbenv.git ~/.rbenv echo 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' >> ~/.bashrc echo 'eval \"$(rbenv init -)\"' >> ~/.bashrc source ~/.bashrc ~/.rbenv/bin/rbenv init git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build Installing Ruby (Based on ruby version) cd Autolab rbenv install ` cat .ruby-version ` Installing SQLite sudo apt-get install sqlite3 libsqlite3-dev Installing MySQL. (If you would just like to test Autolab, then you can skip this step by using SQLite) Following instructions from How to Install MySQL on Ubuntu . sudo apt install mysql-server sudo mysql_secure_installation > There will be a few questions asked during the MySQL setup. * Validate Password Plugin? N * Remove Annonymous Users? Y * Disallow Root Login Remotely? Y * Remove Test Database and Access to it? Y * Reload Privilege Tables Now? Y (If you are using MySQL) Create a new user with access to autolab_test and autolab_development databases. Because a password rather than auth_socket is needed, we need to ensure that user uses mysql_native_password sudo mysql mysql> CREATE USER 'user1' @ 'localhost' IDENTIFIED WITH mysql_native_password BY '<password>' ; mysql> FLUSH PRIVILEGES ; mysql> exit ; Installing Rails cd Autolab gem install bundler rbenv rehash bundle install Initializing Autolab Configs cd Autolab cp config/database.yml.template config/database.yml cp config/school.yml.template config/school.yml cp config/initializers/devise.rb.template config/initializers/devise.rb sed -i \"s/<YOUR-SECRET-KEY>/`bundle exec rake secret`/g\" config/initializers/devise.rb cp config/autogradeConfig.rb.template config/autogradeConfig.rb (Using MySQL) Editing Database YML. Change the and fields in config/database.yml to the username and password that has been set up for the mysql. For example if your username is user1 , and your password is 123456 , then your yml would be development : adapter : mysql2 database : autolab_development pool : 5 username : user1 password : '123456' socket : /var/run/mysqld/ mysqld . sock host : localhost variables : sql_mode : NO_ENGINE_SUBSTITUTION test : adapter : mysql2 database : autolab_test pool : 5 username : user1 password : '123456' socket : /var/run/mysqld/ mysqld . sock host : localhost variables : sql_mode : NO_ENGINE_SUBSTITUTION (Using SQLite) Editing Database YML. Comment out the configurations meant for MySQL in config/database.yml, and insert the following development : adapter : sqlite3 database : db / autolab_development pool : 5 timeout : 5000 test : adapter : sqlite3 database : db / autolab_test pool : 5 timeout : 5000 Granting permissions on the databases. Setting global sql mode is important to relax the rules of mysql when it comes to group by mode ( access mysql using your root first to grant permissions ) mysql> grant all privileges on autolab_development.* to '<username>' @localhost ; mysql> grant all privileges on autolab_test.* to '<username>' @localhost ; mysql> SET GLOBAL sql_mode =( SELECT REPLACE ( @@sql_mode, 'ONLY_FULL_GROUP_BY' , '' )) ; mysql> exit Initializing Autolab Database cd Autolab bundle exec rake db:create bundle exec rake db:reset bundle exec rake db:migrate Populating sample course & students cd Autolab bundle exec rake autolab:populate Run Autolab! cd Autolab bundle exec rails s -p 3000 --binding = 0 .0.0.0 Visit localhost:3000 on your browser to view your local deployment of Autolab, and login with Developer Login Email: \"admin@foo.bar\" Install Tango , the backend autograding service. If you would like to deploy the server, you can try out Phusion Passenger Now you are all set to start using Autolab! Visit the Guide for Instructors and Guide for Lab Authors pages for more info.","title":"Ubuntu 18.04+"},{"location":"#faq","text":"This is a general list of questions that we get often. If you find a solution to an issue not mentioned here, please contact us at autolab-dev@andrew.cmu.edu","title":"FAQ"},{"location":"#ubuntu-script-bugs","text":"If you get the following error Failed to fetch http://dl.google.com/linux/chrome/deb/dists/stable/Release Unable to find expected entry 'main/binary-i386/Packages' in Release file ( Wrong sources.list entry or malformed file ) then follow the solution in this post .","title":"Ubuntu Script Bugs"},{"location":"#where-do-i-find-the-mysql-username-and-password","text":"If this is your first time logging into MySQL, your username is 'root'. You may also need to set the root password: Start the server: sudo /usr/local/mysql/support-files/mysql.server start Set the password: mysqladmin -u root password \"[New_Password]\" If you lost your root password, refer to the MySQL wiki","title":"Where do I find the MySQL username and password?"},{"location":"#bundle-install-errors","text":"This happens as gems get updated. These fixes are gem-specific, but two common ones are eventmachine bundle config build.eventmachine --with-cppflags = -I/usr/local/opt/openssl/include libv8 bundle config build.libv8 --with-system-v8 Run bundle install again If this does not work, another option would be bundle update libv8 Because updating libv8 has dependency on other gems, it might fail due to a need to update other gems. Just do bundle update <gem> according to the error messages until all gems are up to date. Run bundle install again If neither of these works, try exploring this StackOverflow link","title":"Bundle Install Errors"},{"location":"#cant-connect-to-local-mysql-server-through-socket","text":"Make sure you've started the MySQL server and double-check the socket in config/database.yml The default socket location is /tmp/mysql.sock .","title":"Can't connect to local MySQL server through socket"},{"location":"#i-forgot-my-mysql-root-password","text":"You can reset it following the instructions on this Stack Overflow post If mysql complains that the password is expired, follow the instructions on the second answer on this post","title":"I forgot my MySQL root password"},{"location":"#mysql-syntax-error","text":"If you get the following error Mysql2::Error: You have an error in your SQL syntax this may be an issue with using an incompatible version of MySQL. Try switching to MySQL 5.7 if you are currently using a different version.","title":"MySQL Syntax Error"},{"location":"api-interface/","text":"This page details all the endpoints of the Autolab REST API. The client's access token should be included as a parameter to all endpoints. For details on obtaining access tokens, please see the API Overview Routing For version 1 of the API, all endpoints are under the path /api/v1/ . For example, to get user info, send a request to https://<host>/api/v1/user . Request & Response Format All endpoints expect the HTTP GET method unless otherwise specified. All parameters listed below are required unless denoted [OPTIONAL]. All responses are in JSON format. If the request is completed successfully, the HTTP response code will be 200. The reference below details the keys and their respective value types that the client can expect from each endpoint. If an error occurs, the response code will not be 200. The returned JSON will be an object with the key 'error'. Its value will be a string that explains the error. Notes on return value types All datetime formats are strings in the form of YYYY-MM-DDThh:mm:ss.sTZD , e.g. 2017-10-23T04:17:41.000-04:00 , which means 4:17:41 AM on October 23rd, 2017 US Eastern Time. JSON spec only has a 'number' type, but the spec below distinguishes between integers and floats for ease of use in certain languages. If a field does not exist, the value is generally null. Please be sure to check if a value is null before using it. Interface user Get basic user info. Scope: 'user_info' Endpoint: /user Parameters: [none] Responses: key type description first_name string The user's first name. last_name string The user's last name. email string The user's registered email. school string The school the user belongs to. major string The user's major of study. year string The user's year. courses Get all courses currently taking or taken before. Scope: 'user_courses' Endpoint: /courses Parameters: state [OPTIONAL] filter the courses by the state of the course. Should be one of 'disabled', 'completed', 'current', or 'upcoming'. If no state is provided, all courses are returned. Responses: A list of courses. Each course contains: key type description name string The unique url-safe name. display_name string The full name of the course. semester string The semester this course is being offered. late_slack integer The number of seconds after a deadline that the server will still accept a submission and not count it as late. grace_days integer AKA late days. The total number of days (over the entire semester) a student is allowed to submit an assessment late. auth_level string The user's level of access for this course. One of 'student', 'course_assistant', or 'instructor'. assessments Get all the assessments of a course. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments Parameters: [none] Responses: A list of assessments. If the user is only a student of the course, only released assessments are available. Otherwise, all assessments are available. Each assessment contains: key type description name string The unique url-safe name. display_name string The full name of the assessments. start_at datetime The time this assessment is released to students. due_at datetime Students can submit before this time without being penalized or using grace days. end_at datetime Last possible time that students can submit (except those granted extensions.) category_name string Name of the category this assessment belongs to. grading_deadline string Not available if the user is a student. Time after which final scores are included in the gradebook. assessment details Show detailed information of an assessment. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments/{assessment_name} Parameters: [none] Response: key type description name string The unique url-safe name. display_name string The full name of the assessments. description string A short description of the assessment. start_at datetime The time this assessment is released to students. due_at datetime Students can submit before this time without being penalized or using grace days. end_at datetime Last possible time that students can submit (except those granted extensions.) updated_at datetime The last time an update was made to the assessment. max_grace_days integer Maximum number of grace days that a student can spend on this assessment. max_submissions integer The maximum number of times a student can submit the assessment. -1 means unlimited submissions. disable_handins boolean Are handins disallowed by students? category_name string Name of the category this assessment belongs to. group_size integer The maximum size of groups for this assessment. writeup_format string The format of this assessment's writeup. One of 'none', 'url', or 'file'. handout_format string The format of this assessment's handout. One of 'none', 'url', or 'file'. has_scoreboard boolean Does this assessment have a scoreboard? has_autograder boolean Does this assessment use an autograder? grading_deadline string Not available if the user is a student. Time after which final scores are included in the gradebook. problems Get all problems of an assessment. Scope: 'user_courses' Endpoint /courses/{course_name}/assessments/{assessment_name}/problems Parameters: [none] Responses: A list of problems. Each problem contains: key type description name string Full name of the problem. description string Brief description of the problem. max_score float Maximum possible score for this problem. optional boolean Is this problem optional? writeup Get the writeup of an assessment. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments/{assessment_name}/writeup Parameters: [none] Responses: If no writeup exists: key type value writeup string \"none\" If writeup is a url: key type description url string The url of the writeup. If writeup is a file: The file is returned. handout Get the handout of an assessment. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments/{assessment_name}/handout Parameters: [none] Responses: [same as writeup ] submit Make a submission to an assessment. Scope: 'user_submit' Endpoint: POST /courses/{course_name}/assessments/{assessment_name}/submit Parameters: submission[file] The file to submit Note: the name should be the string 'submission[file]' Success Response: key type description version integer The version number of the newly submitted submission. filename string The final filename the submitted file is referred to as. Failure Response: A valid submission request may still fail for many reasons, such as file too large, handins disabled by staff, deadline has passed, etc. When a submission fails, the HTTP response code will not be 200. The response body will include a json with the key 'error'. Its contents will be a user-friendly string that the client may display to the user to explain why the submission has failed. The client must not repeat the request without any modifications. The client is not expected to be able to handle the error automatically. submissions Get all submissions the user has made. Scope: 'user_scores' Endpoint: /courses/{course_name}/assessments/{assessment_name}/submissions Parameters: [none] Response: A list of submissions. Each submission includes: key type description version integer The version number of this submission. filename string The final filename the submitted file is referred to as. created_at datetime The time this submission was made. scores object A dictionary containing the scores of each problem. The keys are the names of the problems, and the value is either the score (a float), or the string 'unreleased' if the score for this problem is not yet released. feedback Get the text feedback given to a problem of a submission. For autograded assessments, the feedback will by default be the autograder feedback, and will be identical for all problems. Scope: 'user_scores' Endpoint: /courses/{course_name}/assessments/{assessment_name}/submissions/{submission_version}/feedback Parameters: problem The name of the problem that the feedback is given to. Response: key type description feedback string The full feedback text for this problem. course_user_data (enrollments) Autolab uses the term course_user_data to represent the users affiliated with a course. It includes all students, course assistants, and instructors of the course. A course_user_data object in the response will be formatted in this form: key type description first_name string The user's first name. last_name string The user's last name. email string The user's registered email. school string The school the user belongs to. major string The user's major of study. year string The user's year. lecture string The user's assigned lecture. section string The user's assigned section. grade_policy string The user's grade policy for this course. nickname string The user's nickname for this course. dropped boolean Is the user marked as dropped from this course? auth_level string The user's level of access for this course. One of 'student', 'course_assistant', or 'instructor'. There are five endpoints related to course_user_data: Index List all course_user_data of a course. Scope: 'instructor_all' Endpoint: GET /courses/{course_name}/course_user_data Parameters: [none] Response: A list of course_user_data objects. Show Show the course_user_data of a particular student in a course. Scope: 'instructor_all' Endpoint: GET /courses/{course_name}/course_user_data/{user_email} Parameters: [none] Response: The requested user's course_user_data object. Create Create a new course_user_data for a course. The user's email is used to uniquely identify the user on Autolab. If the user is not yet a user of Autolab, they need to be registered on Autolab before they can be enrolled in any courses. Scope: 'instructor_all' Endpoint: POST /courses/{course_name}/course_user_data Parameters: key type description email required string The email of the user (to uniquely identify the user). lecture required string The lecture to assign the user to. section required string The section to assign the user to. grade_policy string The user's grade policy (opaque to Autolab). dropped boolean Should the user be marked as dropped? nickname string The nickname to give the user. auth_level required string The level of access this user has for this course. One of 'student', 'course_assistant', or 'instructor'. Response: The newly created course_user_data object. Update Update an existing course_user_data. Scope: 'instructor_all' Endpoint: PUT /courses/{course_name}/course_user_data/{user_email} Parameters: key type description lecture string The lecture to assign the user to. section string The section to assign the user to. grade_policy string The user's grade policy (opaque to Autolab). dropped boolean Should the user be marked as dropped? nickname string The nickname to give the user. auth_level string The level of access this user has for this course. One of 'student', 'course_assistant', or 'instructor'. Response: The newly updated course_user_data object. Destroy Drop a user from a course. Since CUDs are never deleted from the course, this is just a shortcut for updating a user with the dropped attribute set to true. Scope: 'instructor_all' Endpoint: DELETE /courses/{course_name}/course_user_data/{user_email} Parameters: [none] Response: The newly updated course_user_data object.","title":"Reference"},{"location":"api-interface/#routing","text":"For version 1 of the API, all endpoints are under the path /api/v1/ . For example, to get user info, send a request to https://<host>/api/v1/user .","title":"Routing"},{"location":"api-interface/#request-response-format","text":"All endpoints expect the HTTP GET method unless otherwise specified. All parameters listed below are required unless denoted [OPTIONAL]. All responses are in JSON format. If the request is completed successfully, the HTTP response code will be 200. The reference below details the keys and their respective value types that the client can expect from each endpoint. If an error occurs, the response code will not be 200. The returned JSON will be an object with the key 'error'. Its value will be a string that explains the error. Notes on return value types All datetime formats are strings in the form of YYYY-MM-DDThh:mm:ss.sTZD , e.g. 2017-10-23T04:17:41.000-04:00 , which means 4:17:41 AM on October 23rd, 2017 US Eastern Time. JSON spec only has a 'number' type, but the spec below distinguishes between integers and floats for ease of use in certain languages. If a field does not exist, the value is generally null. Please be sure to check if a value is null before using it.","title":"Request &amp; Response Format"},{"location":"api-interface/#interface","text":"","title":"Interface"},{"location":"api-interface/#user","text":"Get basic user info. Scope: 'user_info' Endpoint: /user Parameters: [none] Responses: key type description first_name string The user's first name. last_name string The user's last name. email string The user's registered email. school string The school the user belongs to. major string The user's major of study. year string The user's year.","title":"user"},{"location":"api-interface/#courses","text":"Get all courses currently taking or taken before. Scope: 'user_courses' Endpoint: /courses Parameters: state [OPTIONAL] filter the courses by the state of the course. Should be one of 'disabled', 'completed', 'current', or 'upcoming'. If no state is provided, all courses are returned. Responses: A list of courses. Each course contains: key type description name string The unique url-safe name. display_name string The full name of the course. semester string The semester this course is being offered. late_slack integer The number of seconds after a deadline that the server will still accept a submission and not count it as late. grace_days integer AKA late days. The total number of days (over the entire semester) a student is allowed to submit an assessment late. auth_level string The user's level of access for this course. One of 'student', 'course_assistant', or 'instructor'.","title":"courses"},{"location":"api-interface/#assessments","text":"Get all the assessments of a course. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments Parameters: [none] Responses: A list of assessments. If the user is only a student of the course, only released assessments are available. Otherwise, all assessments are available. Each assessment contains: key type description name string The unique url-safe name. display_name string The full name of the assessments. start_at datetime The time this assessment is released to students. due_at datetime Students can submit before this time without being penalized or using grace days. end_at datetime Last possible time that students can submit (except those granted extensions.) category_name string Name of the category this assessment belongs to. grading_deadline string Not available if the user is a student. Time after which final scores are included in the gradebook.","title":"assessments"},{"location":"api-interface/#assessment-details","text":"Show detailed information of an assessment. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments/{assessment_name} Parameters: [none] Response: key type description name string The unique url-safe name. display_name string The full name of the assessments. description string A short description of the assessment. start_at datetime The time this assessment is released to students. due_at datetime Students can submit before this time without being penalized or using grace days. end_at datetime Last possible time that students can submit (except those granted extensions.) updated_at datetime The last time an update was made to the assessment. max_grace_days integer Maximum number of grace days that a student can spend on this assessment. max_submissions integer The maximum number of times a student can submit the assessment. -1 means unlimited submissions. disable_handins boolean Are handins disallowed by students? category_name string Name of the category this assessment belongs to. group_size integer The maximum size of groups for this assessment. writeup_format string The format of this assessment's writeup. One of 'none', 'url', or 'file'. handout_format string The format of this assessment's handout. One of 'none', 'url', or 'file'. has_scoreboard boolean Does this assessment have a scoreboard? has_autograder boolean Does this assessment use an autograder? grading_deadline string Not available if the user is a student. Time after which final scores are included in the gradebook.","title":"assessment details"},{"location":"api-interface/#problems","text":"Get all problems of an assessment. Scope: 'user_courses' Endpoint /courses/{course_name}/assessments/{assessment_name}/problems Parameters: [none] Responses: A list of problems. Each problem contains: key type description name string Full name of the problem. description string Brief description of the problem. max_score float Maximum possible score for this problem. optional boolean Is this problem optional?","title":"problems"},{"location":"api-interface/#writeup","text":"Get the writeup of an assessment. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments/{assessment_name}/writeup Parameters: [none] Responses: If no writeup exists: key type value writeup string \"none\" If writeup is a url: key type description url string The url of the writeup. If writeup is a file: The file is returned.","title":"writeup"},{"location":"api-interface/#handout","text":"Get the handout of an assessment. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments/{assessment_name}/handout Parameters: [none] Responses: [same as writeup ]","title":"handout"},{"location":"api-interface/#submit","text":"Make a submission to an assessment. Scope: 'user_submit' Endpoint: POST /courses/{course_name}/assessments/{assessment_name}/submit Parameters: submission[file] The file to submit Note: the name should be the string 'submission[file]' Success Response: key type description version integer The version number of the newly submitted submission. filename string The final filename the submitted file is referred to as. Failure Response: A valid submission request may still fail for many reasons, such as file too large, handins disabled by staff, deadline has passed, etc. When a submission fails, the HTTP response code will not be 200. The response body will include a json with the key 'error'. Its contents will be a user-friendly string that the client may display to the user to explain why the submission has failed. The client must not repeat the request without any modifications. The client is not expected to be able to handle the error automatically.","title":"submit"},{"location":"api-interface/#submissions","text":"Get all submissions the user has made. Scope: 'user_scores' Endpoint: /courses/{course_name}/assessments/{assessment_name}/submissions Parameters: [none] Response: A list of submissions. Each submission includes: key type description version integer The version number of this submission. filename string The final filename the submitted file is referred to as. created_at datetime The time this submission was made. scores object A dictionary containing the scores of each problem. The keys are the names of the problems, and the value is either the score (a float), or the string 'unreleased' if the score for this problem is not yet released.","title":"submissions"},{"location":"api-interface/#feedback","text":"Get the text feedback given to a problem of a submission. For autograded assessments, the feedback will by default be the autograder feedback, and will be identical for all problems. Scope: 'user_scores' Endpoint: /courses/{course_name}/assessments/{assessment_name}/submissions/{submission_version}/feedback Parameters: problem The name of the problem that the feedback is given to. Response: key type description feedback string The full feedback text for this problem.","title":"feedback"},{"location":"api-interface/#course_user_data-enrollments","text":"Autolab uses the term course_user_data to represent the users affiliated with a course. It includes all students, course assistants, and instructors of the course. A course_user_data object in the response will be formatted in this form: key type description first_name string The user's first name. last_name string The user's last name. email string The user's registered email. school string The school the user belongs to. major string The user's major of study. year string The user's year. lecture string The user's assigned lecture. section string The user's assigned section. grade_policy string The user's grade policy for this course. nickname string The user's nickname for this course. dropped boolean Is the user marked as dropped from this course? auth_level string The user's level of access for this course. One of 'student', 'course_assistant', or 'instructor'. There are five endpoints related to course_user_data:","title":"course_user_data (enrollments)"},{"location":"api-interface/#index","text":"List all course_user_data of a course. Scope: 'instructor_all' Endpoint: GET /courses/{course_name}/course_user_data Parameters: [none] Response: A list of course_user_data objects.","title":"Index"},{"location":"api-interface/#show","text":"Show the course_user_data of a particular student in a course. Scope: 'instructor_all' Endpoint: GET /courses/{course_name}/course_user_data/{user_email} Parameters: [none] Response: The requested user's course_user_data object.","title":"Show"},{"location":"api-interface/#create","text":"Create a new course_user_data for a course. The user's email is used to uniquely identify the user on Autolab. If the user is not yet a user of Autolab, they need to be registered on Autolab before they can be enrolled in any courses. Scope: 'instructor_all' Endpoint: POST /courses/{course_name}/course_user_data Parameters: key type description email required string The email of the user (to uniquely identify the user). lecture required string The lecture to assign the user to. section required string The section to assign the user to. grade_policy string The user's grade policy (opaque to Autolab). dropped boolean Should the user be marked as dropped? nickname string The nickname to give the user. auth_level required string The level of access this user has for this course. One of 'student', 'course_assistant', or 'instructor'. Response: The newly created course_user_data object.","title":"Create"},{"location":"api-interface/#update","text":"Update an existing course_user_data. Scope: 'instructor_all' Endpoint: PUT /courses/{course_name}/course_user_data/{user_email} Parameters: key type description lecture string The lecture to assign the user to. section string The section to assign the user to. grade_policy string The user's grade policy (opaque to Autolab). dropped boolean Should the user be marked as dropped? nickname string The nickname to give the user. auth_level string The level of access this user has for this course. One of 'student', 'course_assistant', or 'instructor'. Response: The newly updated course_user_data object.","title":"Update"},{"location":"api-interface/#destroy","text":"Drop a user from a course. Since CUDs are never deleted from the course, this is just a shortcut for updating a user with the dropped attribute set to true. Scope: 'instructor_all' Endpoint: DELETE /courses/{course_name}/course_user_data/{user_email} Parameters: [none] Response: The newly updated course_user_data object.","title":"Destroy"},{"location":"api-managing-authorized-apps/","text":"Managing Authorized Apps With the advent of the API, developers can now create new, more versatile and convenient ways of accessing Autolab. What this means for users is that you can now use third-party programs to access Autolab to view assignments, download handouts, and even submit your solutions. Rest assured that all developers and their clients will be manually vetted by our team to ensure quality and safety. However, it is still important that you understand how clients interact with your account. Terminology user: a user of autolab (student/instructor) client: a program that uses the autolab api developer: a person that develops clients Granting access As a user of Autolab, when you want to use a client for the first time, you need to grant access to the client so that it can interact with Autolab for you. Easy Activation : Clients that have access to a web browser (e.g. mobile apps, web apps) will redirect the user directly to the Grant Permissions page on Autolab. Manual Activation : Clients that don't have access to a web browser (e.g. command line programs) will present to the user a 6-digit code (case sensitive) that should be entered on the Autolab website. Note : Third-party clients never ask for your Autolab username or password. Never enter them anywhere else except on the Autolab website (always check the page url before entering your credentials). Manual activation page When you enter the code on the website and click \"Activate\", you will be taken to the Grant Permissions page. API Grant Permissions Page This page shows you all the permissions the client requests. Click 'approve' to grant these permissions to this client. Reviewing your authorized clients As a user, you can review all the clients that you've granted access to on the Manage Authorized Clients page. Click on the menu at the upper right corner, then click on 'Account'. At the bottom of the page you'll find the 'Manage Authorized Clients' link. Manage all the clients that currently have access to your account You can view the permissions that each client has (hover over the icon to see a description of each permission). You can also click 'Revoke' at any time to revoke the access of a client immediately.","title":"Managing Authorized Apps"},{"location":"api-managing-authorized-apps/#managing-authorized-apps","text":"With the advent of the API, developers can now create new, more versatile and convenient ways of accessing Autolab. What this means for users is that you can now use third-party programs to access Autolab to view assignments, download handouts, and even submit your solutions. Rest assured that all developers and their clients will be manually vetted by our team to ensure quality and safety. However, it is still important that you understand how clients interact with your account.","title":"Managing Authorized Apps"},{"location":"api-managing-authorized-apps/#terminology","text":"user: a user of autolab (student/instructor) client: a program that uses the autolab api developer: a person that develops clients","title":"Terminology"},{"location":"api-managing-authorized-apps/#granting-access","text":"As a user of Autolab, when you want to use a client for the first time, you need to grant access to the client so that it can interact with Autolab for you. Easy Activation : Clients that have access to a web browser (e.g. mobile apps, web apps) will redirect the user directly to the Grant Permissions page on Autolab. Manual Activation : Clients that don't have access to a web browser (e.g. command line programs) will present to the user a 6-digit code (case sensitive) that should be entered on the Autolab website. Note : Third-party clients never ask for your Autolab username or password. Never enter them anywhere else except on the Autolab website (always check the page url before entering your credentials). Manual activation page When you enter the code on the website and click \"Activate\", you will be taken to the Grant Permissions page. API Grant Permissions Page This page shows you all the permissions the client requests. Click 'approve' to grant these permissions to this client.","title":"Granting access"},{"location":"api-managing-authorized-apps/#reviewing-your-authorized-clients","text":"As a user, you can review all the clients that you've granted access to on the Manage Authorized Clients page. Click on the menu at the upper right corner, then click on 'Account'. At the bottom of the page you'll find the 'Manage Authorized Clients' link. Manage all the clients that currently have access to your account You can view the permissions that each client has (hover over the icon to see a description of each permission). You can also click 'Revoke' at any time to revoke the access of a client immediately.","title":"Reviewing your authorized clients"},{"location":"api-overview/","text":"Overview The web interface that has served us well for many years is no longer the only way to use Autolab. With the API, developers will be able to help make Autolab more versatile and convenient: Whether it be with a mobile app, a command line tool, a browser extension, or something we've never even thought of. For students and instructors who only plan to use Autolab, try out the Autolab CLI . The Autolab REST API allows developers to create clients that can access features of Autolab on behalf of Autolab users. V1 of the API allows clients to: Access basic user info View courses and assessments Submit to assessments View scores and feedback Manage course enrollments Authorization All endpoints of the Autolab API requires client authentication in the form of an access token. To obtain this access token, clients must obtain authorization from the user. Autolab API uses the standard OAuth2 Authorization Code Grant for user authorization. For clients with no easy access to web browsers (e.g. console apps), an alternative device flow -based authorization method is provided as well. To understand how to authorize and unauthorize clients as a user, go to Managing Authorized Apps Authorization Code Grant Flow The authorization code grant consists of 5 basic steps: Client directs the user to the authorization request endpoint via a web browser. Authorization server (Autolab) authenticates the user. If user grants access to the client, the authorization server provides an \"authorization code\" to the client. Client exchanges the authorization code for an access token from the access token endpoint. Client uses the access token for subsequent requests to the API. The endpoint for obtaining user authorization is /oauth/authorize The endpoint for obtaning access tokens and refresh tokens is oauth/token Section 4.1 of RFC 6749 details the parameters required and the response clients can expect from these endpoints. Autolab API provides a refresh token with every new access token. Once the access token has expired, the client can use the refresh token to obtain a new access token, refresh token pair. Details are also provided in RFC 6749 here . Device Flow For devices that cannot use a web browser to obtain user authorization, the alternative device flow approach circumvents the first 3 steps in the authorization code grant flow. Instead of directing a user to the authorization page directly, the client obtains a user code that the user can enter on the Autolab website from any device. The website then takes the user through the authorization procedure, and returns the authorization code to the client. The client can then use this code to request an access token from the access token endpoint as usual. Note that this is different from the \"device flow\" described in the Internet Draft linked above. Obtaining User Code Request Endpoint: GET /oauth/device_flow_init Parameters: client_id: the client_id obtained when registering the client Success Response: device_code: the verification code used by the client (should be kept secret from the user). user_code: the verification code that should be displayed to the user. verification_uri: the verification uri that the user should use to authorize the client. By default is /activate The latter two should be displayed to the user. Obtaining Authorization Code After asking the user to enter the user code on the verification site, the client should poll the device_flow_authorize endpoint to find out if the user has completed the authorization step. Request Endpoint: GET /oauth/device_flow_authorize Parameters: client_id: the client_id obtained when registering the client device_code: the device_code obtained from the device_flow_init endpoint Failure Responses: 400 Bad Request: {error: authorization_pending} The user has not yet granted or denied the authorization request. Please try again in a while. 429 Too Many Requests: {error: Retry later} The client is polling too frequently. Please wait for a while before polling again. The default rate limit is once every 5 seconds. Success Response: code: the authorization code that should be used to obtain an access token. The client could then perform steps 4 and 5 of the Authorization Code Grant Flow. Getting Started Autolab requires all client applications to be registered clients. Upon registration, a client_id and client_secret pair will be provided to the developers for use in the app as identification to the server. Please contact the administrators of your specific Autolab deployment for registration. Security Concerns Please make sure to keep the client_secret secret. Leaking this code may allow third-parties to impersonate your app. Scopes The scopes of an API client specifies the permissions it has, and must be specified during client registration (can be modified later). Currently, Autolab offers the following scopes for third-party clients: user_info: Access your basic info (e.g. name, email, school, year). user_courses: Access your courses and assessments. user_scores: Access your submissions, scores, and feedback. user_submit: Submit to assessments on your behalf. instructor_all: Access admin options of courses where you are an instructor. Example usages If your app only wants to use the API for quick user authentication, you only need the 'user_info' scope. If you want to develop a mobile client for Autolab that allows students to view their upcoming assessments, you may ask for 'user_info' and 'user_courses'. If you want to write a full desktop client that users can use to submit to assessments and view their grades, you may ask for all 5 scopes. Of course, these are only examples. We can't wait to see what new usages of the API you may come up with! We just recommend that you only ask for the scopes you need as the users will be shown the required scopes during authorization, and it gives them peace of mind to know that an app doesn't ask for excessive permissions.","title":"Overview"},{"location":"api-overview/#overview","text":"The web interface that has served us well for many years is no longer the only way to use Autolab. With the API, developers will be able to help make Autolab more versatile and convenient: Whether it be with a mobile app, a command line tool, a browser extension, or something we've never even thought of. For students and instructors who only plan to use Autolab, try out the Autolab CLI . The Autolab REST API allows developers to create clients that can access features of Autolab on behalf of Autolab users. V1 of the API allows clients to: Access basic user info View courses and assessments Submit to assessments View scores and feedback Manage course enrollments","title":"Overview"},{"location":"api-overview/#authorization","text":"All endpoints of the Autolab API requires client authentication in the form of an access token. To obtain this access token, clients must obtain authorization from the user. Autolab API uses the standard OAuth2 Authorization Code Grant for user authorization. For clients with no easy access to web browsers (e.g. console apps), an alternative device flow -based authorization method is provided as well. To understand how to authorize and unauthorize clients as a user, go to Managing Authorized Apps","title":"Authorization"},{"location":"api-overview/#authorization-code-grant-flow","text":"The authorization code grant consists of 5 basic steps: Client directs the user to the authorization request endpoint via a web browser. Authorization server (Autolab) authenticates the user. If user grants access to the client, the authorization server provides an \"authorization code\" to the client. Client exchanges the authorization code for an access token from the access token endpoint. Client uses the access token for subsequent requests to the API. The endpoint for obtaining user authorization is /oauth/authorize The endpoint for obtaning access tokens and refresh tokens is oauth/token Section 4.1 of RFC 6749 details the parameters required and the response clients can expect from these endpoints. Autolab API provides a refresh token with every new access token. Once the access token has expired, the client can use the refresh token to obtain a new access token, refresh token pair. Details are also provided in RFC 6749 here .","title":"Authorization Code Grant Flow"},{"location":"api-overview/#device-flow","text":"For devices that cannot use a web browser to obtain user authorization, the alternative device flow approach circumvents the first 3 steps in the authorization code grant flow. Instead of directing a user to the authorization page directly, the client obtains a user code that the user can enter on the Autolab website from any device. The website then takes the user through the authorization procedure, and returns the authorization code to the client. The client can then use this code to request an access token from the access token endpoint as usual. Note that this is different from the \"device flow\" described in the Internet Draft linked above.","title":"Device Flow"},{"location":"api-overview/#obtaining-user-code","text":"Request Endpoint: GET /oauth/device_flow_init Parameters: client_id: the client_id obtained when registering the client Success Response: device_code: the verification code used by the client (should be kept secret from the user). user_code: the verification code that should be displayed to the user. verification_uri: the verification uri that the user should use to authorize the client. By default is /activate The latter two should be displayed to the user.","title":"Obtaining User Code"},{"location":"api-overview/#obtaining-authorization-code","text":"After asking the user to enter the user code on the verification site, the client should poll the device_flow_authorize endpoint to find out if the user has completed the authorization step. Request Endpoint: GET /oauth/device_flow_authorize Parameters: client_id: the client_id obtained when registering the client device_code: the device_code obtained from the device_flow_init endpoint Failure Responses: 400 Bad Request: {error: authorization_pending} The user has not yet granted or denied the authorization request. Please try again in a while. 429 Too Many Requests: {error: Retry later} The client is polling too frequently. Please wait for a while before polling again. The default rate limit is once every 5 seconds. Success Response: code: the authorization code that should be used to obtain an access token. The client could then perform steps 4 and 5 of the Authorization Code Grant Flow.","title":"Obtaining Authorization Code"},{"location":"api-overview/#getting-started","text":"Autolab requires all client applications to be registered clients. Upon registration, a client_id and client_secret pair will be provided to the developers for use in the app as identification to the server. Please contact the administrators of your specific Autolab deployment for registration. Security Concerns Please make sure to keep the client_secret secret. Leaking this code may allow third-parties to impersonate your app.","title":"Getting Started"},{"location":"api-overview/#scopes","text":"The scopes of an API client specifies the permissions it has, and must be specified during client registration (can be modified later). Currently, Autolab offers the following scopes for third-party clients: user_info: Access your basic info (e.g. name, email, school, year). user_courses: Access your courses and assessments. user_scores: Access your submissions, scores, and feedback. user_submit: Submit to assessments on your behalf. instructor_all: Access admin options of courses where you are an instructor. Example usages If your app only wants to use the API for quick user authentication, you only need the 'user_info' scope. If you want to develop a mobile client for Autolab that allows students to view their upcoming assessments, you may ask for 'user_info' and 'user_courses'. If you want to write a full desktop client that users can use to submit to assessments and view their grades, you may ask for all 5 scopes. Of course, these are only examples. We can't wait to see what new usages of the API you may come up with! We just recommend that you only ask for the scopes you need as the users will be shown the required scopes during authorization, and it gives them peace of mind to know that an app doesn't ask for excessive permissions.","title":"Scopes"},{"location":"command-line-interface/","text":"Autolab Command Line Interface To help showcase the capabilities of the API, we developed autolab-cli: A first-party command line client that serves as both a practical tool for users of Autolab, as well as a reference design for developers intending to use the API in their own programs. The cli includes features like downloading and submitting assignments from the terminal, viewing problems, and getting submission feedback. Note to CMU Students: This cli binary has already been installed on the andrew machines as autolab . Obtaining authorization Make sure you have the cli installed by running autolab in your terminal. If you see the usage instructions you're good to go. Otherwise, ask your school admin to install the cli from the Autolab CLI Repository . To setup autolab-cli with your Autolab account, run autolab setup . This will initiate a manual activation. What you'll see when you run autolab setup Once you approve the client on the Autolab website, the client will respond telling you that authorization was successful. You should be able to use the client from now on. If at any point you want to reset the client, run autolab setup -f and you'll be asked to re-authorize the client from a clean state. To deauthorize any client that you've given permission to, look at how to Manage Authorized Apps . Viewing your courses and assessments To view your current courses, run $ autolab courses This will show you a list of ongoing courses in the form unique_name (Display name) . You should use the 'unique_name' of each course when interacting with autolab-cli. To view the assessments of a course, run $ autolab asmts <course_unique_name> This will show you a list of assessments in the same unique_name (Display name) format. Downloading an assessment To start working on an assessment, go to a directory where you usually put your work, and run $ autolab download <course_unique_name>:<asmt_unique_name> This will create a directory with the assessment name in your current directory, and download the handout and writeup in it. This new directory is called an 'assessment directory'. Whenever you're inside an assessment directory, autolab-cli will respond according to the context. For example, when you're inside an assessment directory, you can run $ autolab problems This will show you the problems of this assessment. Submitting solutions To submit to an assessment inside an assessment directory, run $ autolab submit <filename> Yep, it's that easy. Viewing scores To view the scores you got, run $ autolab scores The scores command will only return scores for those submissions that are made via this client. This is a privacy constraint of the Autolab API. To view the feedback you got, run $ autolab feedback Advanced features You can learn more about each sub-command by running $ autolab <sub-command> -h This will reveal other flags you may be able to use with each command. For example, you can call all of the context-dependent commands outside of an assessment directory by providing the <course_unique_name>:<asmt_unique_name> pair. We hope this speeds up your workflow! If you find any problems, please file an issue on the Autolab CLI Repository .","title":"Command Line Interface"},{"location":"command-line-interface/#autolab-command-line-interface","text":"To help showcase the capabilities of the API, we developed autolab-cli: A first-party command line client that serves as both a practical tool for users of Autolab, as well as a reference design for developers intending to use the API in their own programs. The cli includes features like downloading and submitting assignments from the terminal, viewing problems, and getting submission feedback. Note to CMU Students: This cli binary has already been installed on the andrew machines as autolab .","title":"Autolab Command Line Interface"},{"location":"command-line-interface/#obtaining-authorization","text":"Make sure you have the cli installed by running autolab in your terminal. If you see the usage instructions you're good to go. Otherwise, ask your school admin to install the cli from the Autolab CLI Repository . To setup autolab-cli with your Autolab account, run autolab setup . This will initiate a manual activation. What you'll see when you run autolab setup Once you approve the client on the Autolab website, the client will respond telling you that authorization was successful. You should be able to use the client from now on. If at any point you want to reset the client, run autolab setup -f and you'll be asked to re-authorize the client from a clean state. To deauthorize any client that you've given permission to, look at how to Manage Authorized Apps .","title":"Obtaining authorization"},{"location":"command-line-interface/#viewing-your-courses-and-assessments","text":"To view your current courses, run $ autolab courses This will show you a list of ongoing courses in the form unique_name (Display name) . You should use the 'unique_name' of each course when interacting with autolab-cli. To view the assessments of a course, run $ autolab asmts <course_unique_name> This will show you a list of assessments in the same unique_name (Display name) format.","title":"Viewing your courses and assessments"},{"location":"command-line-interface/#downloading-an-assessment","text":"To start working on an assessment, go to a directory where you usually put your work, and run $ autolab download <course_unique_name>:<asmt_unique_name> This will create a directory with the assessment name in your current directory, and download the handout and writeup in it. This new directory is called an 'assessment directory'. Whenever you're inside an assessment directory, autolab-cli will respond according to the context. For example, when you're inside an assessment directory, you can run $ autolab problems This will show you the problems of this assessment.","title":"Downloading an assessment"},{"location":"command-line-interface/#submitting-solutions","text":"To submit to an assessment inside an assessment directory, run $ autolab submit <filename> Yep, it's that easy.","title":"Submitting solutions"},{"location":"command-line-interface/#viewing-scores","text":"To view the scores you got, run $ autolab scores The scores command will only return scores for those submissions that are made via this client. This is a privacy constraint of the Autolab API. To view the feedback you got, run $ autolab feedback","title":"Viewing scores"},{"location":"command-line-interface/#advanced-features","text":"You can learn more about each sub-command by running $ autolab <sub-command> -h This will reveal other flags you may be able to use with each command. For example, you can call all of the context-dependent commands outside of an assessment directory by providing the <course_unique_name>:<asmt_unique_name> pair. We hope this speeds up your workflow! If you find any problems, please file an issue on the Autolab CLI Repository .","title":"Advanced features"},{"location":"dave/","text":"Dave On the historical and cultural significance of \"Dave\", and the consequences thereof Okay, so we use the word \"dave\" as a variable name in a few places because we think it's funny (and come on, you do too). The word \"dave\", though, doesn't really describe anything except Dave, and changing the variable names to something more descriptive would require changing a lot of stuff across a ton of our repositories. So, uhhh, writing docs is easier. Here's what \"dave\" means: On the interactions of Tango and Autolab By now, we know that Tango and Autolab are two different things (Or at least, I'm pretty sure they are). But, when Tango and Autolab exchange submissions, they need a value to identify unique submissions. Hence \"dave\".","title":"Dave"},{"location":"dave/#dave","text":"","title":"Dave"},{"location":"dave/#on-the-historical-and-cultural-significance-of-dave-and-the-consequences-thereof","text":"Okay, so we use the word \"dave\" as a variable name in a few places because we think it's funny (and come on, you do too). The word \"dave\", though, doesn't really describe anything except Dave, and changing the variable names to something more descriptive would require changing a lot of stuff across a ton of our repositories. So, uhhh, writing docs is easier. Here's what \"dave\" means:","title":"On the historical and cultural significance of \"Dave\", and the consequences thereof"},{"location":"dave/#on-the-interactions-of-tango-and-autolab","text":"By now, we know that Tango and Autolab are two different things (Or at least, I'm pretty sure they are). But, when Tango and Autolab exchange submissions, they need a value to identify unique submissions. Hence \"dave\".","title":"On the interactions of Tango and Autolab"},{"location":"features/","text":"Autolab Features Documentation This guide details the usage of features in Autolab. Features Documented (Work in Progress): Formatted Feedback Scoreboards Embedded Forms Annotations MOSS Plagiarism Detection Formatted Feedback Autograding feedback plays an important role in a student's Autolab experience. Good feedback provided by autograders can really enhance a student's learning. As of Summer 2020, Autolab includes the formatted feedback feature by Jala Alamin . The feature was originally introduced in Washington State University Vancouver's version of Autolab. Using formatted feedback requires a prior understanding of how Autolab's autograders work, as per the Guide for Lab Authors . The formatted feedback feature is an optional extension of the default feedback. It comes in a staged fashion, allowing differing levels of adoption. The next few sections are meant to be read in order, with each following section introducing a more complex usage of the formatted feedback feature than the previous. Experimenting the hellocat example code is another way to familiarize with the formatted feedback. Default Feedback By only outputting the autoresult ( autoresult is the JSON string that needs to be outputted on the last line of stdout, as mentioned in the Guide for Lab Authors ), the default feedback format will automatically be used. { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 } } Autolab will simply display the raw output as produced by the autograder Semantic Feedback (Minimal) By adding an additional JSON string before the autoresult , as follows { \"_presentation\" : \"semantic\" } { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 } } we can invoke the semantic layout, which will display the both the raw output and a formatted table of scores. Semantic Feedback with Test Cases By further describing the additional JSON string, we can introduce test stages to the formatted feedback, which we can use to indicate to the student the test cases that have passed and/or failed. Actual JSON to be outputted { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Test Results\" ], \"Test Results\" : { \"Build\" : { \"passed\" : true }, \"Run\" : { \"passed\" : true }}} { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} Prettified JSON (for reference only) { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Test Results\" ], \"Test Results\" : { \"Build\" : { \"passed\" : true }, \"Run\" : { \"passed\" : true } } } { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} We would add [\"Test Results\"] to the stages key. Then we would add to the corresponding Test Results key an object containing all the test case results. In this case Build and Run were used, but you can use other names for the test cases as well. Semantic Feedback (Multi-Stage) Using the same manner in which we add a Test Stage in the previous section, we can adapt it to create as many stages as we want. The following example has three different stages, namely Build , Test and Timing , but you can use other names for the stages as well. Actual JSON to be outputted { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Build\" , \"Test\" , \"Timing\" ], \"Test\" : { \"Add Things\" : { \"passed\" : true }, \"Return Values\" : { \"passed\" : false , \"hint\" : \"You need to return 1\" }}, \"Build\" : { \"compile\" : { \"passed\" : true }, \"link\" : { \"passed\" : true }}, \"Timing\" : { \"Stage 1 (ms)\" : 10 , \"Stage 2 (ms)\" : 20 }} { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} Prettified JSON (for reference only) { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Build\" , \"Test\" , \"Timing\" ], \"Test\" : { \"Add Things\" : { \"passed\" : true }, \"Return Values\" : { \"passed\" : false , \"hint\" : \"You need to return 1\" } }, \"Build\" : { \"compile\" : { \"passed\" : true }, \"link\" : { \"passed\" : true } }, \"Timing\" : { \"Stage 1 (ms)\" : 10 , \"Stage 2 (ms)\" : 20 } } { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} We would add the stages we want into the stages array. Then we would add those corresponding stages as a separate key, with each of them holding their own set of test case results. We are also able to provide hints if the student gets the particular test case wrong by adding a hint key to the test case. Scoreboards Scoreboards are created by the output of Autograders . They anonomously rank students submitted assignments inspiring health competition and desire to improve. They are simple and highly customizable. Scoreboard's can be added/edited on the edit assessment screen ( /courses/<course>/assessments/<assessment>/edit ). In general, scoreboards are configured using a JSON string. Default Scoreboard The default scoreboard displays the total problem scores, followed by each individual problem score, sorted in descending order by the total score. Custom Scoreboards Autograded assignments have the option of creating custom scoreboards. You can specify your own custom scoreboard using a JSON column specification. The column spec consists of a \"scoreboard\" object, which is an array of JSON objects, where each object describes a column. Example: a scoreboard with one column, called Score . { \"scoreboard\" : [{ \"hdr\" : \"Score\" }] } A custom scoreboard sorts the first three columns, from left to right, in descending order. You can change the default sort order for a particular column by adding an optional \"asc:1\" element to its hash. Example: Scoreboard with two columns, \"Score\" and \"Ops\", with \"Score\" sorted descending, and then \"Ops\" ascending: { \"scoreboard\" : [{ \"hdr\" : \"Score\" }, { \"hdr\" : \"Ops\" , \"asc\" : 1 }] } Scoreboard Entries The values for each row in a custom scoreboard come directly from a scoreboard array object in the autoresult string produced by the Tango, the autograder. Example: Autoresult returning the score (97) for a single autograded problem called autograded , and a scoreboard entry with two columns: the autograded score ( Score ) and the number of operations ( Ops ): { \"scores\" : { \"autograded\" : 97 }, \"scoreboard\" : [ 97 , 128 ] } For more information on how to use Autograders and Scoreboards together, visit the Guide for Lab Authors . Embedded Forms This feature allows an instructor to create an assessment which does not require a file submission on the part of the student. Instead, when an assessment is created, the hand-in page for that assessment will display an HTML form of the instructor\u2019s design. When the student submits the form, the information is sent directly in JSON format to the Tango grading server for evaluation. Tango Required Tango is needed to use this feature. Please install Tango and connect it to Autolab before proceeding. Creating an Embedded Form Create an HTML file with a combination of the following elements. The HTML file need only include form elements, because it will automatically be wrapped in a <form></form> block when it is rendered on the page. In order for the JSON string (the information passed to the grader) to be constructed properly, your form elements must follow the following conventions: A unique name attribute A value attribute which corresponds to the correct answer to the question (unless it is a text field or text area) HTML Form Reference: Text Field (For short responses) < input type = \"\u201ctext\u201d\" name = \"\u201cquestion-1\u201d\" /> Text Area (For coding questions) < textarea name = \"\u201cquestion-2\u201d\" style = \"\u201cwidth:100%\u201d\" /> Radio Button (For multiple choice) < div class = \"row\" > < label > < input name = \"question-3\" type = \"radio\" value = \"object\" id = \"q3-1\" /> < span > Object </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"boolean\" id = \"q3-2\" /> < span > Boolean </ span > </ label > </ div > Dropdown (For multiple choice or select all that apply) < select multiple name = \"question-4\" > < option value = \"1\" > Option 1 </ option > < option value = \"2\" > Option 2 </ option > < option value = \"3\" > Option 3 </ option > </ select > Example Form (shown in screenshot above) < div > < h6 > What's your name? </ h6 > < input type = \"text\" name = \"question-1\" id = \"q1\" /> </ div > < div > < h6 > Which year are you? </ h6 > < div class = \"row\" > < label > < input name = \"question-2\" type = \"radio\" value = \"freshman\" id = \"q3-1\" /> < span > Freshman </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"sophomore\" id = \"q3-2\" /> < span > Sophomore </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"junior\" id = \"q3-3\" /> < span > Junior </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"senior\" id = \"q3-4\" /> < span > Senior </ span > </ label > </ div > </ div > < div > < h6 > What's your favorite language? </ h6 > < select name = \"question-3\" id = \"q4\" > < option value = \"C\" > C </ option > < option value = \"Python\" > Python </ option > < option value = \"Java\" > Java </ option > </ select > </ div > Navigate to the Basic section of editing an assessment ( /courses/<course>/assessments/<assessment>/edit ), check the check box, and upload the HTML file. Ensure you submit the form by clicking Save at the bottom of the page. Grading an Embedded Form When a student submits a form, the form data is sent to Tango in the form of a JSON string in the file out.txt. In your grading script, parse the contents of out.txt as a JSON object. The JSON object will be a key-value pair data structure, so you can access the students response string ( value ) by its unique key (the name attribute). For the example form shown above, the JSON object will be as follows: { \"utf8\" : \"\u2713\" , \"authenticity_token\" : \"LONGAUTHTOKEN\" , \"submission[embedded_quiz_form_answer]\" : \"\" , \"question-1\" : \"John Smith\" , \"question-2\" : \"junior\" , \"question-3\" : \"Python\" , \"integrity_checkbox\" : \"1\" } Use this information to do any processing you need in Tango.If you find any problems, please file an issue on the Autolab Github . Annotations Annotations is a feature introduced as part of the Speedgrader update to Autolab. It allows instructors and TAs to quickly leave comments and grade code at the same time. Hover over any line of the code and click on the green arrow, and the annotation form will appear. Add the comment, adjust the score, and select the targetted problem. Non-Autograded Problems Only Note that annotations can only be added to non-autograded problems. Specifically, a problem is non-autograded if there is no assigned score for that problem in the json outputted by the autograder Scoring Behavior There are two intended ways for course instructors to use the add annotation features. Deductions from maximum, or additions from zero. Deductions from maximum Set a max_score either programmatically, or under Edit Assessment > Problems for the particular non-autograded question. Then when the grader is viewing the code, add negative score, such as -5 into the score field, to deduct from the maximum. This use case is preferred when grading based on a rubric, and the score is deducted for each mistake. The maximum score can be 0 if the deductions are meant to be penalties, such as for poor code style or violation of library interfaces. Additions from zero Set a max_score either programmatically, or under Edit Assessment > Problems for the particular non-autograded question to 0 . When the grader is viewing the code, add positive scores, such as 5 to the score field, to add to the score. This use case is preferred when giving out bonus points. Interaction with Gradesheet We have kept the ability the edit the scores in the gradesheet, as we understand that there are instances in which editing the gradesheet directly is much more efficient and/or needed. However, this leads to an unintended interaction with the annotations. In particular, modifications on the gradesheet itself will override all changes made to a problem by annotations, but the annotations made will still remain. A example would be, if the max_score of a problem is 10 . A grader adds an annotation with -5 score to that problem (so the score is now 10-5=5 ). Then if the same/another grader changes the score to 8 on the gradesheet, the final score would be 8 . Recommendation It is much preferred to grade using annotations whenever possible, as it provides a better experience for the students who will be able to identify the exact line at which the mistake is made. Gradesheet should be used in situations where the modification is non-code related. MOSS Plagiarism Detection Installation MOSS (Measure Of Software Similarity) is a system for checking for plagiarism. MOSS can be setup on Autolab as follows: Obtain the script for MOSS based on the instructions given in https://theory.stanford.edu/~aiken/moss/ . Create a directory called vendor at the root of your Autolab installation, i.e bash cd <autolab_root> mkdir -p vendor Copy the moss script into the vendor directory and name it mossnet bash mv <path_to_moss_script> vendor/mossnet","title":"Features"},{"location":"features/#autolab-features-documentation","text":"This guide details the usage of features in Autolab. Features Documented (Work in Progress): Formatted Feedback Scoreboards Embedded Forms Annotations MOSS Plagiarism Detection","title":"Autolab Features Documentation"},{"location":"features/#formatted-feedback","text":"Autograding feedback plays an important role in a student's Autolab experience. Good feedback provided by autograders can really enhance a student's learning. As of Summer 2020, Autolab includes the formatted feedback feature by Jala Alamin . The feature was originally introduced in Washington State University Vancouver's version of Autolab. Using formatted feedback requires a prior understanding of how Autolab's autograders work, as per the Guide for Lab Authors . The formatted feedback feature is an optional extension of the default feedback. It comes in a staged fashion, allowing differing levels of adoption. The next few sections are meant to be read in order, with each following section introducing a more complex usage of the formatted feedback feature than the previous. Experimenting the hellocat example code is another way to familiarize with the formatted feedback.","title":"Formatted Feedback"},{"location":"features/#default-feedback","text":"By only outputting the autoresult ( autoresult is the JSON string that needs to be outputted on the last line of stdout, as mentioned in the Guide for Lab Authors ), the default feedback format will automatically be used. { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 } } Autolab will simply display the raw output as produced by the autograder","title":"Default Feedback"},{"location":"features/#semantic-feedback-minimal","text":"By adding an additional JSON string before the autoresult , as follows { \"_presentation\" : \"semantic\" } { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 } } we can invoke the semantic layout, which will display the both the raw output and a formatted table of scores.","title":"Semantic Feedback (Minimal)"},{"location":"features/#semantic-feedback-with-test-cases","text":"By further describing the additional JSON string, we can introduce test stages to the formatted feedback, which we can use to indicate to the student the test cases that have passed and/or failed. Actual JSON to be outputted { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Test Results\" ], \"Test Results\" : { \"Build\" : { \"passed\" : true }, \"Run\" : { \"passed\" : true }}} { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} Prettified JSON (for reference only) { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Test Results\" ], \"Test Results\" : { \"Build\" : { \"passed\" : true }, \"Run\" : { \"passed\" : true } } } { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} We would add [\"Test Results\"] to the stages key. Then we would add to the corresponding Test Results key an object containing all the test case results. In this case Build and Run were used, but you can use other names for the test cases as well.","title":"Semantic Feedback with Test Cases"},{"location":"features/#semantic-feedback-multi-stage","text":"Using the same manner in which we add a Test Stage in the previous section, we can adapt it to create as many stages as we want. The following example has three different stages, namely Build , Test and Timing , but you can use other names for the stages as well. Actual JSON to be outputted { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Build\" , \"Test\" , \"Timing\" ], \"Test\" : { \"Add Things\" : { \"passed\" : true }, \"Return Values\" : { \"passed\" : false , \"hint\" : \"You need to return 1\" }}, \"Build\" : { \"compile\" : { \"passed\" : true }, \"link\" : { \"passed\" : true }}, \"Timing\" : { \"Stage 1 (ms)\" : 10 , \"Stage 2 (ms)\" : 20 }} { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} Prettified JSON (for reference only) { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Build\" , \"Test\" , \"Timing\" ], \"Test\" : { \"Add Things\" : { \"passed\" : true }, \"Return Values\" : { \"passed\" : false , \"hint\" : \"You need to return 1\" } }, \"Build\" : { \"compile\" : { \"passed\" : true }, \"link\" : { \"passed\" : true } }, \"Timing\" : { \"Stage 1 (ms)\" : 10 , \"Stage 2 (ms)\" : 20 } } { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} We would add the stages we want into the stages array. Then we would add those corresponding stages as a separate key, with each of them holding their own set of test case results. We are also able to provide hints if the student gets the particular test case wrong by adding a hint key to the test case.","title":"Semantic Feedback (Multi-Stage)"},{"location":"features/#scoreboards","text":"Scoreboards are created by the output of Autograders . They anonomously rank students submitted assignments inspiring health competition and desire to improve. They are simple and highly customizable. Scoreboard's can be added/edited on the edit assessment screen ( /courses/<course>/assessments/<assessment>/edit ). In general, scoreboards are configured using a JSON string.","title":"Scoreboards"},{"location":"features/#default-scoreboard","text":"The default scoreboard displays the total problem scores, followed by each individual problem score, sorted in descending order by the total score.","title":"Default Scoreboard"},{"location":"features/#custom-scoreboards","text":"Autograded assignments have the option of creating custom scoreboards. You can specify your own custom scoreboard using a JSON column specification. The column spec consists of a \"scoreboard\" object, which is an array of JSON objects, where each object describes a column. Example: a scoreboard with one column, called Score . { \"scoreboard\" : [{ \"hdr\" : \"Score\" }] } A custom scoreboard sorts the first three columns, from left to right, in descending order. You can change the default sort order for a particular column by adding an optional \"asc:1\" element to its hash. Example: Scoreboard with two columns, \"Score\" and \"Ops\", with \"Score\" sorted descending, and then \"Ops\" ascending: { \"scoreboard\" : [{ \"hdr\" : \"Score\" }, { \"hdr\" : \"Ops\" , \"asc\" : 1 }] }","title":"Custom Scoreboards"},{"location":"features/#scoreboard-entries","text":"The values for each row in a custom scoreboard come directly from a scoreboard array object in the autoresult string produced by the Tango, the autograder. Example: Autoresult returning the score (97) for a single autograded problem called autograded , and a scoreboard entry with two columns: the autograded score ( Score ) and the number of operations ( Ops ): { \"scores\" : { \"autograded\" : 97 }, \"scoreboard\" : [ 97 , 128 ] } For more information on how to use Autograders and Scoreboards together, visit the Guide for Lab Authors .","title":"Scoreboard Entries"},{"location":"features/#embedded-forms","text":"This feature allows an instructor to create an assessment which does not require a file submission on the part of the student. Instead, when an assessment is created, the hand-in page for that assessment will display an HTML form of the instructor\u2019s design. When the student submits the form, the information is sent directly in JSON format to the Tango grading server for evaluation. Tango Required Tango is needed to use this feature. Please install Tango and connect it to Autolab before proceeding.","title":"Embedded Forms"},{"location":"features/#creating-an-embedded-form","text":"Create an HTML file with a combination of the following elements. The HTML file need only include form elements, because it will automatically be wrapped in a <form></form> block when it is rendered on the page. In order for the JSON string (the information passed to the grader) to be constructed properly, your form elements must follow the following conventions: A unique name attribute A value attribute which corresponds to the correct answer to the question (unless it is a text field or text area) HTML Form Reference: Text Field (For short responses) < input type = \"\u201ctext\u201d\" name = \"\u201cquestion-1\u201d\" /> Text Area (For coding questions) < textarea name = \"\u201cquestion-2\u201d\" style = \"\u201cwidth:100%\u201d\" /> Radio Button (For multiple choice) < div class = \"row\" > < label > < input name = \"question-3\" type = \"radio\" value = \"object\" id = \"q3-1\" /> < span > Object </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"boolean\" id = \"q3-2\" /> < span > Boolean </ span > </ label > </ div > Dropdown (For multiple choice or select all that apply) < select multiple name = \"question-4\" > < option value = \"1\" > Option 1 </ option > < option value = \"2\" > Option 2 </ option > < option value = \"3\" > Option 3 </ option > </ select > Example Form (shown in screenshot above) < div > < h6 > What's your name? </ h6 > < input type = \"text\" name = \"question-1\" id = \"q1\" /> </ div > < div > < h6 > Which year are you? </ h6 > < div class = \"row\" > < label > < input name = \"question-2\" type = \"radio\" value = \"freshman\" id = \"q3-1\" /> < span > Freshman </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"sophomore\" id = \"q3-2\" /> < span > Sophomore </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"junior\" id = \"q3-3\" /> < span > Junior </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"senior\" id = \"q3-4\" /> < span > Senior </ span > </ label > </ div > </ div > < div > < h6 > What's your favorite language? </ h6 > < select name = \"question-3\" id = \"q4\" > < option value = \"C\" > C </ option > < option value = \"Python\" > Python </ option > < option value = \"Java\" > Java </ option > </ select > </ div > Navigate to the Basic section of editing an assessment ( /courses/<course>/assessments/<assessment>/edit ), check the check box, and upload the HTML file. Ensure you submit the form by clicking Save at the bottom of the page.","title":"Creating an Embedded Form"},{"location":"features/#grading-an-embedded-form","text":"When a student submits a form, the form data is sent to Tango in the form of a JSON string in the file out.txt. In your grading script, parse the contents of out.txt as a JSON object. The JSON object will be a key-value pair data structure, so you can access the students response string ( value ) by its unique key (the name attribute). For the example form shown above, the JSON object will be as follows: { \"utf8\" : \"\u2713\" , \"authenticity_token\" : \"LONGAUTHTOKEN\" , \"submission[embedded_quiz_form_answer]\" : \"\" , \"question-1\" : \"John Smith\" , \"question-2\" : \"junior\" , \"question-3\" : \"Python\" , \"integrity_checkbox\" : \"1\" } Use this information to do any processing you need in Tango.If you find any problems, please file an issue on the Autolab Github .","title":"Grading an Embedded Form"},{"location":"features/#annotations","text":"Annotations is a feature introduced as part of the Speedgrader update to Autolab. It allows instructors and TAs to quickly leave comments and grade code at the same time. Hover over any line of the code and click on the green arrow, and the annotation form will appear. Add the comment, adjust the score, and select the targetted problem. Non-Autograded Problems Only Note that annotations can only be added to non-autograded problems. Specifically, a problem is non-autograded if there is no assigned score for that problem in the json outputted by the autograder","title":"Annotations"},{"location":"features/#scoring-behavior","text":"There are two intended ways for course instructors to use the add annotation features. Deductions from maximum, or additions from zero. Deductions from maximum Set a max_score either programmatically, or under Edit Assessment > Problems for the particular non-autograded question. Then when the grader is viewing the code, add negative score, such as -5 into the score field, to deduct from the maximum. This use case is preferred when grading based on a rubric, and the score is deducted for each mistake. The maximum score can be 0 if the deductions are meant to be penalties, such as for poor code style or violation of library interfaces. Additions from zero Set a max_score either programmatically, or under Edit Assessment > Problems for the particular non-autograded question to 0 . When the grader is viewing the code, add positive scores, such as 5 to the score field, to add to the score. This use case is preferred when giving out bonus points.","title":"Scoring Behavior"},{"location":"features/#interaction-with-gradesheet","text":"We have kept the ability the edit the scores in the gradesheet, as we understand that there are instances in which editing the gradesheet directly is much more efficient and/or needed. However, this leads to an unintended interaction with the annotations. In particular, modifications on the gradesheet itself will override all changes made to a problem by annotations, but the annotations made will still remain. A example would be, if the max_score of a problem is 10 . A grader adds an annotation with -5 score to that problem (so the score is now 10-5=5 ). Then if the same/another grader changes the score to 8 on the gradesheet, the final score would be 8 . Recommendation It is much preferred to grade using annotations whenever possible, as it provides a better experience for the students who will be able to identify the exact line at which the mistake is made. Gradesheet should be used in situations where the modification is non-code related.","title":"Interaction with Gradesheet"},{"location":"features/#moss-plagiarism-detection-installation","text":"MOSS (Measure Of Software Similarity) is a system for checking for plagiarism. MOSS can be setup on Autolab as follows: Obtain the script for MOSS based on the instructions given in https://theory.stanford.edu/~aiken/moss/ . Create a directory called vendor at the root of your Autolab installation, i.e bash cd <autolab_root> mkdir -p vendor Copy the moss script into the vendor directory and name it mossnet bash mv <path_to_moss_script> vendor/mossnet","title":"MOSS Plagiarism Detection Installation"},{"location":"instructors/","text":"Guide for Instructors This document provides instructors with a brief overview of the basic ideas and capabilities of the Autolab system. It's meant to be read from beginning to end the first time. Users Users are either instructors , course assistants , or students . Instructors have full permissions. Course assistants are only allowed to enter grades. Students see only their own work. Each user is uniquely identified by their email address. You can change the permissions for a particular user at any time. Note that some instructors opt to give some or all of their TAs instructor status. Roster The roster holds the list of users. You can add and remove users one at a time, or in bulk by uploading a CSV file in the general Autolab format: Semester,email,last_name,first_name,school,major,year,grading_policy,courseNumber,courseLecture,section or in the format that is exported by the CMU S3 service: \"Semester\",\"Course\",\"Section\",\"Lecture\",\"Mini\",\"Last Name\",\"First Name\",\"MI\",\"AndrewID\",\"Email\",\"College\",\"Department\",... Attention CMU Instructors: S3 lists each student twice: once in a lecture roster, which lists the lecture number (e.g., 1, 2,...) in the section field, and once in a section roster, which lists the section letter (e.g., A, B,...) in the section field. Be careful not to import the lecture roster. Instead, export and upload each section individually. Or you can export everything from S3 with a single action, edit out the roster entries for the lecture(s), and then upload a single file to Autolab with all of the sections. For the bulk upload, you can choose to either: add any new students in the roster file to the Autolab roster, or to update the Autolab roster by marking students missing from roster files as dropped . Instructors and course assistants are never marked as dropped. User accounts are never deleted. Students marked as dropped can still see their work, but cannot submit new work and do not appear on the instructor gradebook. Instructors can change the dropped status of a student at any time. Once a student is added to the roster for a course, then that course becomes visible to the student when they visit the Autolab site. A student can be enrolled in an arbitrary number of Autolab courses. Labs (Assessments) A lab (or assessment ) is broadly defined as a submission set; it is anything that your students make submissions (handins) for. This could be a programming assignment, a typed homework, or even an in-class exam. You can create labs from scratch, or reuse them from previous semesters. See the companion Guide For Lab Authors for info on writing and installing labs. Assessment Categories You can tag each assessment with an arbitrary user-defined category , e.g., \"Lab\", \"Exam\", \"Homework\". Autograders and Scoreboards Labs can be autograded or not, at your disrcretion. When a student submits to an autograded lab, Autolab runs an instructor-supplied autograder program that assigns scores to one or more problems associated with the lab. Autograded labs can have an optional scoreboard that shows (anonymized) results in real-time. See the companion Guide For Lab Authors for details on writing autograded labs with scoreboards. Important Dates A lab has a start date , due date , end date and grading deadline . The link to a lab becomes visible to students after the start date (it's always visible to instructors). Students can submit until the due date without penalty or consuming grace days. Submission is turned off after the end date. Grades are included in the gradebook's category and course averages only after the grading deadline. Handins Once an assessment is live (past the start date), students can begin submitting handins, where each handin is a single file, which can be either a text file or an archive file (e.g., mm.c , handin.tar ). Penalties and Extensions You can set penalties for late handins, set hard limits on the number of handins, or set soft limits that penalize excessive handins on a sliding scale. You can also give a student an extension that extends the due dates and end dates for that student. Grace Days Autolab provides support for a late handin policy based on grace days . Each student has a semester-long budget of grace days that are automatically applied if they handin after the due date. Each late day consumes one of the budgeted grace days. The Autolab system keeps track of the number of grace days that have been used by each student to date. If students run out of grace days and handin late, then there is a fixed late penalty (possibly zero) that can be set by the instructor. Problems Each lab contains at least one problem , defined by the instructor, with some point value. Each problem has a name (e.g., \"Prob1\", \"Style\") that is unique for the lab (although different labs can have the same problem names). Submissions Once an assessment is live (past the start date), students can begin making submissions (handins), where each submission is a single file. Grades Grades come in a number of different forms: Problem scores: These are scalar values (possibly negative) assigned per problem per submission, either manually by a human grader after the end date, or automatically by an autograder after each submission. Problem scores can also be uploaded (imported) in bulk from a CSV file. Assessment raw score: By default, the raw score is the sum of the individual problem scores, before any penalties are applied. You can override the default raw score calculation. See below. Assessment total score: The total score is the raw score, plus any late penalties, plus any instructor tweaks . Category averages: This is the average for a particular student over all assessments in a specific instructor-defined category such as \"Labs, or \"Exams\". By default the category average is the arithmetic mean of all assessment total scores, but it can be overwridden. See below. Course Average: By default, the course average is average of all category averages, but can be overidden. See below. Submissions can be classified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No Grade\" submission will show up in the gradebook as NG and a zero will be used when calculating averages. An \"Excused\" submission will show up in the gradebook as EXC and will not be used when calculating averages. Overriding Raw Score Calculations Autolab computes raw scores for a lab with a Ruby function called raw_score . The default is the sum of the individual problem scores. But you can change this by providing your own raw_score function in <labname>.rb file. For example, to override the raw_score calculation for a lab called malloclab , you might add the following raw_score function to malloclab/malloclab.rb : # In malloclab/malloclab.rb file def raw_score ( score ) perfindex = score [ \"Autograded Score\" ]. to_f () heap = score [ \"Heap Checker\" ]. to_f () style = score [ \"Style\" ]. to_f () deduct = score [ \"CorrectnessDeductions\" ]. to_f () perfpoints = perfindex # perfindex below 50 gets autograded score of 0. if perfindex < 50 . 0 then perfpoints = 0 else perfpoints = perfindex end return perfpoints + heap + style + deduct end This particular lab has four problems called \"Autograded Score\", \"Heap Checker\", \"Style\", and \"CorrectnessDeductions\". An \"Autograded Score\" less than 50 is set to zero when the raw score is calculated. Note: To make this change live, you must select the \"Reload config file\" option on the malloclab page. Overriding Category and Course Averages The average for a category foo is calculated by a default Ruby function called fooAverage , which you can override in the course.rb file. For example, in our course, we prefer to report the \"average\" as the total number of normalized points (out of 100) that the student has accrued so far. This helps them understand where they stand in the class, e.g., \"Going into the final exam (worth 30 normalized points), I have 60 normalized points, so the only way to get an A is to get 100% on the final.\" Here's the Ruby function for category \"Lab\": # In course.rb file def LabAverage ( user ) pts = ( user [ 'datalab' ]. to_f () / 63 . 0 ) * 6 . 0 + ( user [ 'bomblab' ]. to_f () / 70 . 0 ) * 5 . 0 + ( user [ 'attacklab' ]. to_f () / 100 . 0 ) * 4 . 0 + ( user [ 'cachelab' ]. to_f () / 60 . 0 ) * 7 . 0 + ( user [ 'tshlab' ]. to_f () / 110 . 0 ) * 8 . 0 + ( user [ 'malloclab' ]. to_f () / 120 . 0 ) * 12 . 0 + ( user [ 'proxylab' ]. to_f () / 100 . 0 ) * 8 . 0 return pts . to_f () . round ( 2 ) end In this case, labs are worth a total of 50/100 normalized points. The assessment called datalab is graded out of a total of 63 points and is worth 6/50 normalized points. Here is the Ruby function for category \"Exam\": # In course.rb file def ExamAverage ( user ) pts = (( user [ 'midterm' ]. to_f () / 60 . 0 ) * 20 . 0 ) + (( user [ 'final' ]. to_f () / 80 . 0 ) * 30 . 0 ) return pts . to_f () . round ( 2 ) end In this case, exams are worth 50/100 normalized points. The assessment called midterm is graded out of total of 60 points and is worth 20/50 normalized points. The course average is computed by a default Ruby function called courseAverage , which can be overridden by the course.rb file in the course directory. Here is the function for our running example: # In course.rb file def courseAverage ( user ) pts = user [ 'catLab' ]. to_f () + user [ 'catExam' ]. to_f () return pts . to_f () . round ( 2 ) end In this course, the course average is the sum of the category averages for \"Lab\" and \"Exam\". Note: To make these changes live, you must select \"Reload course config file\" on the \"Manage course\" page. Handin History For each lab, students can view all of their submissions, including any source code, and the problem scores, penalties, and total scores associated with those submissions, via the handin history page. Gradesheet The gradesheet (not to be confused with the gradebook ) is the workhorse grading tool. Each assessment has a separate gradesheet with the following features: Provides an interface for manually entering problem scores (and problem feedback) for the most recent submmission from each student. Provides an interface for viewing and annotating the submitted code. Displays the problem scores for the most recent submission for each student, summarizes any late penalties, and computes the total score. Provides a link to each student's handin history. Gradebook The gradebook comes in two forms. The student gradebook displays the grades for a particular student, including total scores for each assessment, category averages, and the course average. The instructor gradebook is a table that displays the grades for the most recent submission of each student, including assessment total scores, category averages and course average. For the gradebook calculations, submissions are classified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No Grade\" submission will show up in the gradebook as NG and a zero will be used when calculating averages. An \"Excused\" submission will show up in the gradebook as EXC and will not be used when calculating averages. Releasing Grades Manually assigned grades are by default not released, and therefore not visible to students. You can release grades on an individual basis while grading, or release all available grades in bulk by using the \"Release all grades\" option. You can also reverse this process using the \"Withdraw all grades\" option. (The word \"withdraw\" is perhaps unfortunate. No grades are ever deleted. They are simply withdrawn from the student's view.)","title":"Guide for Instructors"},{"location":"instructors/#guide-for-instructors","text":"This document provides instructors with a brief overview of the basic ideas and capabilities of the Autolab system. It's meant to be read from beginning to end the first time.","title":"Guide for Instructors"},{"location":"instructors/#users","text":"Users are either instructors , course assistants , or students . Instructors have full permissions. Course assistants are only allowed to enter grades. Students see only their own work. Each user is uniquely identified by their email address. You can change the permissions for a particular user at any time. Note that some instructors opt to give some or all of their TAs instructor status.","title":"Users"},{"location":"instructors/#roster","text":"The roster holds the list of users. You can add and remove users one at a time, or in bulk by uploading a CSV file in the general Autolab format: Semester,email,last_name,first_name,school,major,year,grading_policy,courseNumber,courseLecture,section or in the format that is exported by the CMU S3 service: \"Semester\",\"Course\",\"Section\",\"Lecture\",\"Mini\",\"Last Name\",\"First Name\",\"MI\",\"AndrewID\",\"Email\",\"College\",\"Department\",... Attention CMU Instructors: S3 lists each student twice: once in a lecture roster, which lists the lecture number (e.g., 1, 2,...) in the section field, and once in a section roster, which lists the section letter (e.g., A, B,...) in the section field. Be careful not to import the lecture roster. Instead, export and upload each section individually. Or you can export everything from S3 with a single action, edit out the roster entries for the lecture(s), and then upload a single file to Autolab with all of the sections. For the bulk upload, you can choose to either: add any new students in the roster file to the Autolab roster, or to update the Autolab roster by marking students missing from roster files as dropped . Instructors and course assistants are never marked as dropped. User accounts are never deleted. Students marked as dropped can still see their work, but cannot submit new work and do not appear on the instructor gradebook. Instructors can change the dropped status of a student at any time. Once a student is added to the roster for a course, then that course becomes visible to the student when they visit the Autolab site. A student can be enrolled in an arbitrary number of Autolab courses.","title":"Roster"},{"location":"instructors/#labs-assessments","text":"A lab (or assessment ) is broadly defined as a submission set; it is anything that your students make submissions (handins) for. This could be a programming assignment, a typed homework, or even an in-class exam. You can create labs from scratch, or reuse them from previous semesters. See the companion Guide For Lab Authors for info on writing and installing labs.","title":"Labs (Assessments)"},{"location":"instructors/#assessment-categories","text":"You can tag each assessment with an arbitrary user-defined category , e.g., \"Lab\", \"Exam\", \"Homework\".","title":"Assessment Categories"},{"location":"instructors/#autograders-and-scoreboards","text":"Labs can be autograded or not, at your disrcretion. When a student submits to an autograded lab, Autolab runs an instructor-supplied autograder program that assigns scores to one or more problems associated with the lab. Autograded labs can have an optional scoreboard that shows (anonymized) results in real-time. See the companion Guide For Lab Authors for details on writing autograded labs with scoreboards.","title":"Autograders and Scoreboards"},{"location":"instructors/#important-dates","text":"A lab has a start date , due date , end date and grading deadline . The link to a lab becomes visible to students after the start date (it's always visible to instructors). Students can submit until the due date without penalty or consuming grace days. Submission is turned off after the end date. Grades are included in the gradebook's category and course averages only after the grading deadline.","title":"Important Dates"},{"location":"instructors/#handins","text":"Once an assessment is live (past the start date), students can begin submitting handins, where each handin is a single file, which can be either a text file or an archive file (e.g., mm.c , handin.tar ).","title":"Handins"},{"location":"instructors/#penalties-and-extensions","text":"You can set penalties for late handins, set hard limits on the number of handins, or set soft limits that penalize excessive handins on a sliding scale. You can also give a student an extension that extends the due dates and end dates for that student.","title":"Penalties and Extensions"},{"location":"instructors/#grace-days","text":"Autolab provides support for a late handin policy based on grace days . Each student has a semester-long budget of grace days that are automatically applied if they handin after the due date. Each late day consumes one of the budgeted grace days. The Autolab system keeps track of the number of grace days that have been used by each student to date. If students run out of grace days and handin late, then there is a fixed late penalty (possibly zero) that can be set by the instructor.","title":"Grace Days"},{"location":"instructors/#problems","text":"Each lab contains at least one problem , defined by the instructor, with some point value. Each problem has a name (e.g., \"Prob1\", \"Style\") that is unique for the lab (although different labs can have the same problem names).","title":"Problems"},{"location":"instructors/#submissions","text":"Once an assessment is live (past the start date), students can begin making submissions (handins), where each submission is a single file.","title":"Submissions"},{"location":"instructors/#grades","text":"Grades come in a number of different forms: Problem scores: These are scalar values (possibly negative) assigned per problem per submission, either manually by a human grader after the end date, or automatically by an autograder after each submission. Problem scores can also be uploaded (imported) in bulk from a CSV file. Assessment raw score: By default, the raw score is the sum of the individual problem scores, before any penalties are applied. You can override the default raw score calculation. See below. Assessment total score: The total score is the raw score, plus any late penalties, plus any instructor tweaks . Category averages: This is the average for a particular student over all assessments in a specific instructor-defined category such as \"Labs, or \"Exams\". By default the category average is the arithmetic mean of all assessment total scores, but it can be overwridden. See below. Course Average: By default, the course average is average of all category averages, but can be overidden. See below. Submissions can be classified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No Grade\" submission will show up in the gradebook as NG and a zero will be used when calculating averages. An \"Excused\" submission will show up in the gradebook as EXC and will not be used when calculating averages.","title":"Grades"},{"location":"instructors/#overriding-raw-score-calculations","text":"Autolab computes raw scores for a lab with a Ruby function called raw_score . The default is the sum of the individual problem scores. But you can change this by providing your own raw_score function in <labname>.rb file. For example, to override the raw_score calculation for a lab called malloclab , you might add the following raw_score function to malloclab/malloclab.rb : # In malloclab/malloclab.rb file def raw_score ( score ) perfindex = score [ \"Autograded Score\" ]. to_f () heap = score [ \"Heap Checker\" ]. to_f () style = score [ \"Style\" ]. to_f () deduct = score [ \"CorrectnessDeductions\" ]. to_f () perfpoints = perfindex # perfindex below 50 gets autograded score of 0. if perfindex < 50 . 0 then perfpoints = 0 else perfpoints = perfindex end return perfpoints + heap + style + deduct end This particular lab has four problems called \"Autograded Score\", \"Heap Checker\", \"Style\", and \"CorrectnessDeductions\". An \"Autograded Score\" less than 50 is set to zero when the raw score is calculated. Note: To make this change live, you must select the \"Reload config file\" option on the malloclab page.","title":"Overriding Raw Score Calculations"},{"location":"instructors/#overriding-category-and-course-averages","text":"The average for a category foo is calculated by a default Ruby function called fooAverage , which you can override in the course.rb file. For example, in our course, we prefer to report the \"average\" as the total number of normalized points (out of 100) that the student has accrued so far. This helps them understand where they stand in the class, e.g., \"Going into the final exam (worth 30 normalized points), I have 60 normalized points, so the only way to get an A is to get 100% on the final.\" Here's the Ruby function for category \"Lab\": # In course.rb file def LabAverage ( user ) pts = ( user [ 'datalab' ]. to_f () / 63 . 0 ) * 6 . 0 + ( user [ 'bomblab' ]. to_f () / 70 . 0 ) * 5 . 0 + ( user [ 'attacklab' ]. to_f () / 100 . 0 ) * 4 . 0 + ( user [ 'cachelab' ]. to_f () / 60 . 0 ) * 7 . 0 + ( user [ 'tshlab' ]. to_f () / 110 . 0 ) * 8 . 0 + ( user [ 'malloclab' ]. to_f () / 120 . 0 ) * 12 . 0 + ( user [ 'proxylab' ]. to_f () / 100 . 0 ) * 8 . 0 return pts . to_f () . round ( 2 ) end In this case, labs are worth a total of 50/100 normalized points. The assessment called datalab is graded out of a total of 63 points and is worth 6/50 normalized points. Here is the Ruby function for category \"Exam\": # In course.rb file def ExamAverage ( user ) pts = (( user [ 'midterm' ]. to_f () / 60 . 0 ) * 20 . 0 ) + (( user [ 'final' ]. to_f () / 80 . 0 ) * 30 . 0 ) return pts . to_f () . round ( 2 ) end In this case, exams are worth 50/100 normalized points. The assessment called midterm is graded out of total of 60 points and is worth 20/50 normalized points. The course average is computed by a default Ruby function called courseAverage , which can be overridden by the course.rb file in the course directory. Here is the function for our running example: # In course.rb file def courseAverage ( user ) pts = user [ 'catLab' ]. to_f () + user [ 'catExam' ]. to_f () return pts . to_f () . round ( 2 ) end In this course, the course average is the sum of the category averages for \"Lab\" and \"Exam\". Note: To make these changes live, you must select \"Reload course config file\" on the \"Manage course\" page.","title":"Overriding Category and Course Averages"},{"location":"instructors/#handin-history","text":"For each lab, students can view all of their submissions, including any source code, and the problem scores, penalties, and total scores associated with those submissions, via the handin history page.","title":"Handin History"},{"location":"instructors/#gradesheet","text":"The gradesheet (not to be confused with the gradebook ) is the workhorse grading tool. Each assessment has a separate gradesheet with the following features: Provides an interface for manually entering problem scores (and problem feedback) for the most recent submmission from each student. Provides an interface for viewing and annotating the submitted code. Displays the problem scores for the most recent submission for each student, summarizes any late penalties, and computes the total score. Provides a link to each student's handin history.","title":"Gradesheet"},{"location":"instructors/#gradebook","text":"The gradebook comes in two forms. The student gradebook displays the grades for a particular student, including total scores for each assessment, category averages, and the course average. The instructor gradebook is a table that displays the grades for the most recent submission of each student, including assessment total scores, category averages and course average. For the gradebook calculations, submissions are classified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No Grade\" submission will show up in the gradebook as NG and a zero will be used when calculating averages. An \"Excused\" submission will show up in the gradebook as EXC and will not be used when calculating averages.","title":"Gradebook"},{"location":"instructors/#releasing-grades","text":"Manually assigned grades are by default not released, and therefore not visible to students. You can release grades on an individual basis while grading, or release all available grades in bulk by using the \"Release all grades\" option. You can also reverse this process using the \"Withdraw all grades\" option. (The word \"withdraw\" is perhaps unfortunate. No grades are ever deleted. They are simply withdrawn from the student's view.)","title":"Releasing Grades"},{"location":"lab/","text":"Guide for Lab Authors This guide explains how to create autograded programming assignments (labs) for the Autolab system. Writing Autograders An autograder is a program that takes a student's work as input, and generates some quantitative evaluation of that work as output. The student's work consists of one or more source files written in an arbitrary programming language. The autograder processes these files and generates arbitrary text lines on stdout. The last text line on stdout must be a JSON string, called an autoresult , that assigns an autograded score to one or more problems, and optionally, generates the scoreboard entries for this submission. The JSON autoresult is a \"scores\" hash that assigns a numerical score to one or more problems, and an optional \"scoreboard\" array that provides the scoreboard entries for this submission. For example, { \"scores\" : { \"Prob1\" : 10 , \"Prob2\" : 5 } } assigns 10 points to \"Prob1\" and 5 points to \"Prob2\" for this submission. The names of the problems must exactly match the names of the problems for this lab on the Autolab web site. Not all problems need to be autograded. For example, there might be a problem for this assessment called \"Style\" that you grade manually after the due date. If you used the Autolab web site to configure a scoreboard for this lab with three columns called \"Prob1\", \"Prob2\", and \"Total\", then the autoresult might be: { \"scores\" : { \"Prob1\" : 10 , \"Prob2\" : 5 }, \"scoreboard\" : [ 10 , 5 , 15 ] } By convention, an autograder accepts an optional -A command line argument that tells it to emit the JSON autoresult. So if you run the autograder outside of the context of Autolab, you can suppress the autoresult line by calling the autograder without the -A argument. One of the nice properties of Autolab autograders is that they can be written and tested offline, without requiring any interaction with Autolab. Writing autograders is not easy, but the fact that they can be developed offline allows you to develop and test them in your own familiar computing environment. Installing Autograded Labs After you've written and tested the autograder, you then use the Autolab web site to create the autograded lab. Autolab supports creating new labs from scratch, or reusing labs from previous semesters. We'll describe each of these in turn. Creating an Autograded Lab from Scratch Step 1: Create the new lab. Create a new lab by clicking the \"Install Assessment\" button and choosing \"Option 1: Create a New Assessment from Scratch.\" For course <course> and lab <lab> , this will create a lab directory in the Autolab file hierarchy called courses/<course>/<lab> . This initial directory contains a couple of config files and a directory called <lab>/handin that will contain all of the student handin files. In general, you should never modify any of these. Attention CMU Lab Authors At CMU, the lab directory is called /afs/cs/academic/class/<course>/autolab/<lab> . For example: /afs/cs/academic/class/15213-f16/autolab/foo is the lab directory for the lab named foo for the Fall 2016 instance of 15-213. All lab-related files must go in this autolab directory to avoid permissions issues. Step 2: Configure the lab for autograding. Using the \"Edit Assessment\" page, turn on autograding for this lab by selecting \"Add Autograder.\" You will be asked for the name of the image to be used for autograding this lab. The default image distributed with Autolab is an Ubuntu image called autograding_image . If your class needs different software, then you or your facilities staff will need to update the default image or create a new one. Attention CMU Lab Authors The default autograding image at CMU is called rhel.img and is a copy of the software on the CMU Andrew machines ( linux.andrew.cmu.edu ). If you need custom software installed, please send mail to autolab-help@andrew.cmu.edu. If you want a scoreboard, you should select \"Add Scoreboard,\" which will allow you to specify the number of columns and their names. The \"Add Scoreboard\" page contains a tutorial on how to do this. You'll also need to define the names and point values for all the problems in this lab, including the autograded ones. Each student submission is a single file, either a text source file or an archive file containing multiple files and directories. You'll need to specify the base name for the student submission files (e.g., mm.c , handin.tar ). Step 3: Add the required autograding files. For an autograded lab, Autolab expects the following two autograding files in the lab directory: autograde-Makefile : runs the autograder on a student submission. autograde.tar : contains all of the files (except for the student handin file) that are needed for autograding. Each time a student submits their work or an instructor requests a regrade, Autolab copies the student handin file, along with the two autograding files, to an empty directory on an autograding instance , renames the student handin file to base name (e.g., hello.c, handin.tar), renames autograde-Makefile to Makefile , executes the command make on the autograding instance, and finally captures the stdout generated by the autograder, and parses the resulting JSON autoresult to determine the autograded scores. Importing an Autograded Lab from a Previous Semester If you've created a lab for a course in a previous semester and have access to the lab directory (as we do at CMU via AFS), you can import the lab into your current course by copying the lab directory from the previous course to the current course, cleaning out the handin directory, then visiting the \"Install Assessment\" page and selecting \"Option 2: Import an existing assessment from the file system.\" Autolab will give you a list of all of the directories that appear to be uninstalled labs, from which you can select your particular lab. If you don't have access to the lab directory, another option is to import a lab from a tarball that was created by running \"Export assessment\" in an instance of a lab from a previous semester. Visit the \"Install Assessment\" page and select \"Option 3: Import an existing assessment from tarball.\" This will upload the tarball, create a new lab directory by expanding the tarball, and then import the directory. Example: Hello Lab In this section we'll look at the simplest possible autograded lab we could imagine, called, appropriately enough, the Hello Lab (with tarball ), which is stored in a lab directory called hello in the Autolab github repo. While it's trivial, it illustrates all of the aspects of developing an autograded lab, and provides a simple example that you can use for sanity testing on your Autolab installation. In this lab, students are asked to write a version of the K&R \"hello, world\" program, called hello.c . The autograder simply checks that the submitted hello.c program compiles and runs with an exit status of zero. If so, the submission gets 100 points. Otherwise it gets 0 points. Directory Structure Autolab expects to find the autograde-Makefile and autograde.tar files in the hello lab directory, but otherwise places no constraints on the contents and organization of this directory. However, based on our experience, we strongly recommend a directory structure with the following form: hello/README : # Basic files created by the lab author Makefile Builds the lab from src/ README autograde-Makefile Makefile that runs the autograder src/ Contains all src files and solutions test-autograder/ For testing autograder offline writeup/ Lab writeup that students view from Autolab # Files created by running make hello-handout/ The directory that is handed out to students, created using files from src/. hello-handout.tar Archive of hello-handout directory autograde.tar File that is copied to the autograding instance (along with autograde-Makefile and student handin file) # Files created and managed by Autolab handin/ All students handin files hello.rb Config file hello.yml Database properties that persist from semester to semester log.txt Log of autograded submissions The key idea with this directory structure is to place all code for the lab in the src directory, including the autograding code and any starter code handed out to students in the handout directory ( hello-handout.tar in this example). Keeping all hard state in the src directory helps limit inconsistencies. The main makefile creates hello-handout by copying files from src , and then tars it up: hello/Makefile : # # Makefile to manage the example Hello Lab # # Get the name of the lab directory LAB = $( notdir $( PWD )) all : handout handout - tarfile handout : # Rebuild the handout directory that students download ( rm -rf $( LAB ) -handout ; mkdir $( LAB ) -handout ) cp -p src/Makefile-handout $( LAB ) -handout/Makefile cp -p src/README-handout $( LAB ) -handout/README cp -p src/hello.c-handout $( LAB ) -handout/hello.c cp -p src/driver.sh $( LAB ) -handout handout-tarfile : handout # Build *-handout.tar and autograde.tar tar cvf $( LAB ) -handout.tar $( LAB ) -handout cp -p $( LAB ) -handout.tar autograde.tar clean : # Clean the entire lab directory tree. Note that you can run # \"make clean; make\" at any time while the lab is live with no # adverse effects. rm -f *~ *.tar ( cd src ; make clean ) ( cd test-autograder ; make clean ) rm -rf $( LAB ) -handout rm -f autograde.tar # # CAREFULL!!! This will delete all student records in the logfile and # in the handin directory. Don't run this once the lab has started. # Use it to clean the directory when you are starting a new version # of the lab from scratch, or when you are debugging the lab prior # to releasing it to the students. # cleanallfiles : # Reset the lab from scratch. make clean rm -f log.txt rm -rf handin/* Filenames are disambiguated by appending -handout , which is stripped when they are copied to the handout directory. For example, src/hello.c is the instructor's solution file, and src/hello.c-handout is the starter code that is given to the students in hello-handout/hello.c . And src/README is the README for the src directory and src/README-handout is the README that is handed out to students in hello-handout/README . To build the lab, type make clean; make . You can do this as often as you like while the lab is live with no adverse effects. However, be careful to never type make cleanallfiles while the lab is live; this should only be done before the lab goes live; never during or after. Source Directory The hello/src/ directory contains all of the code files for the Hello Lab, including the files that are handed out to students: hello/src/README : # Autograder and solution files Makefile Makefile and ... README ... README for this directory driver . sh * Autograder hello . c Solution hello . c file # Files that are handed out to students Makefile - handout Makefile and ... README - handout ... README handed out to students hello . c - handout Blank hello . c file handed out to students Handout Directory The hello/hello-handout/ directory contains the files that the students will use to work on the lab. It contains no hard state, and is populated entirely with files from hello/src : hello/hello-handout/README : For this lab , you should write a tiny C program , called \"hello.c\" , that prints \"hello, world\" to stdout and then indicates success by exiting with a status of zero . To test your work: $ make clean ; make ; ./ hello To run the same autograder that Autolab will use when you submit: $ ./ driver . sh Files: README This file Makefile Compiles hello . c driver . sh Autolab autograder hello . c Empty C file that you will edit hello/hello-handout/Makefile contains the rules that compile the student source code: # Student makefile for the Hello Lab all : gcc hello.c -o hello clean : rm -rf *~ hello To compile and run their code, students type: $ make clean ; make $ ./hello Autograder The autograder for the Hello Lab is a trivially simple bash script called driver.sh that compiles and runs hello.c and verifies that it returns with an exit status of zero: hello/src/driver.sh #!/bin/bash # driver.sh - The simplest autograder we could think of. It checks # that students can write a C program that compiles, and then # executes with an exit status of zero. # Usage: ./driver.sh # Compile the code echo \"Compiling hello.c\" ( make clean ; make ) status = $? if [ ${ status } -ne 0 ] ; then echo \"Failure: Unable to compile hello.c (return status = ${ status } )\" echo \"{\\\"scores\\\": {\\\"Correctness\\\": 0}}\" exit fi # Run the code echo \"Running ./hello\" ./hello status = $? if [ ${ status } -eq 0 ] ; then echo \"Success: ./hello runs with an exit status of 0\" echo \"{\\\"scores\\\": {\\\"Correctness\\\": 100}}\" else echo \"Failure: ./hello fails or returns nonzero exit status of ${ status } \" echo \"{\\\"scores\\\": {\\\"Correctness\\\": 0}}\" fi exit For example: $ ./driver.sh # Compiling hello.c # rm -rf *~ hello # gcc hello.c -o hello # Running ./hello # Hello, world # Success: ./hello runs with an exit status of 0 # {\"scores\": {\"Correctness\": 100}} Notice that the autograder expects the hello lab on the Autolab front-end to have been defined with a problem called \"Correctness\", with a maximum value of 100 points. If you forget to define the problems listed in the JSON autoresult, scores will still be logged, but they won't be posted to the database. Required Autograding Files Autolab requires two autograding files called autograde.tar , which contains all of the code required by the autograder, and autograde-Makefile , which runs the autograder on the autograding image when each submission is graded. For the Hello Lab, autograde.tar is simply a copy of the hello-handout.tar file that is handed out to students. And here is the corresponding hello/autograde-makefile : all : tar xvf autograde.tar cp hello.c hello-handout ( cd hello-handout ; ./driver.sh ) clean : rm -rf *~ hello-handout The makefile expands autograde.tar into hello-handout , copies hello.c (the submission file) into hello-handout , changes directory to hello-handout , builds the autograder, and then runs it. Test Directory For our labs, we like to setup a test directory (called test-autograder in this example), that allows us to test our autograde-Makefile and autograde-tar files by simulating Autolab's behavior on the autograding instance. The test-autograder directory has the following form: $ cd test-autograder $ ls -l # total 3 # lrwxr-xr-x 1 droh users 21 Aug 4 16:43 Makefile -> autograde-Makefile # lrwxr-xr-x 1 droh users 16 Aug 4 16:43 autograde.tar -> autograde.tar # -rw-rw-r-- 1 droh users 113 Aug 4 16:44 hello.c To simulate Autolab's behavior on an autograding instance: $ cd test-autograder && make clean && make # Running ./hello # Hello, world # Success: ./hello runs with an exit status of 0 # {\"scores\": {\"Correctness\": 100}} Writeup directory The hello/writeup contains the detailed lab writeup, either html or pdf file, that students can download from the Autolab front end. FAQ Why is Autolab not displaying my stdout output? Autolab always shows the stdout output of running make, even when the program crashed or timed out. However, when it does crash and the expected autoresult json string is not appended to the output, parsing of the last line will fail. If this happens, any stdout output that is longer than 10,000 lines will be discarded (Note that this limit does not apply when the autoresult json is valid). Is there a way to formatting the feedback provided to the students Yes there is, by using the formatted feedback feature .","title":"Guide for Lab Authors"},{"location":"lab/#guide-for-lab-authors","text":"This guide explains how to create autograded programming assignments (labs) for the Autolab system.","title":"Guide for Lab Authors"},{"location":"lab/#writing-autograders","text":"An autograder is a program that takes a student's work as input, and generates some quantitative evaluation of that work as output. The student's work consists of one or more source files written in an arbitrary programming language. The autograder processes these files and generates arbitrary text lines on stdout. The last text line on stdout must be a JSON string, called an autoresult , that assigns an autograded score to one or more problems, and optionally, generates the scoreboard entries for this submission. The JSON autoresult is a \"scores\" hash that assigns a numerical score to one or more problems, and an optional \"scoreboard\" array that provides the scoreboard entries for this submission. For example, { \"scores\" : { \"Prob1\" : 10 , \"Prob2\" : 5 } } assigns 10 points to \"Prob1\" and 5 points to \"Prob2\" for this submission. The names of the problems must exactly match the names of the problems for this lab on the Autolab web site. Not all problems need to be autograded. For example, there might be a problem for this assessment called \"Style\" that you grade manually after the due date. If you used the Autolab web site to configure a scoreboard for this lab with three columns called \"Prob1\", \"Prob2\", and \"Total\", then the autoresult might be: { \"scores\" : { \"Prob1\" : 10 , \"Prob2\" : 5 }, \"scoreboard\" : [ 10 , 5 , 15 ] } By convention, an autograder accepts an optional -A command line argument that tells it to emit the JSON autoresult. So if you run the autograder outside of the context of Autolab, you can suppress the autoresult line by calling the autograder without the -A argument. One of the nice properties of Autolab autograders is that they can be written and tested offline, without requiring any interaction with Autolab. Writing autograders is not easy, but the fact that they can be developed offline allows you to develop and test them in your own familiar computing environment.","title":"Writing Autograders"},{"location":"lab/#installing-autograded-labs","text":"After you've written and tested the autograder, you then use the Autolab web site to create the autograded lab. Autolab supports creating new labs from scratch, or reusing labs from previous semesters. We'll describe each of these in turn.","title":"Installing Autograded Labs"},{"location":"lab/#creating-an-autograded-lab-from-scratch","text":"","title":"Creating an Autograded Lab from Scratch"},{"location":"lab/#step-1-create-the-new-lab","text":"Create a new lab by clicking the \"Install Assessment\" button and choosing \"Option 1: Create a New Assessment from Scratch.\" For course <course> and lab <lab> , this will create a lab directory in the Autolab file hierarchy called courses/<course>/<lab> . This initial directory contains a couple of config files and a directory called <lab>/handin that will contain all of the student handin files. In general, you should never modify any of these. Attention CMU Lab Authors At CMU, the lab directory is called /afs/cs/academic/class/<course>/autolab/<lab> . For example: /afs/cs/academic/class/15213-f16/autolab/foo is the lab directory for the lab named foo for the Fall 2016 instance of 15-213. All lab-related files must go in this autolab directory to avoid permissions issues.","title":"Step 1: Create the new lab."},{"location":"lab/#step-2-configure-the-lab-for-autograding","text":"Using the \"Edit Assessment\" page, turn on autograding for this lab by selecting \"Add Autograder.\" You will be asked for the name of the image to be used for autograding this lab. The default image distributed with Autolab is an Ubuntu image called autograding_image . If your class needs different software, then you or your facilities staff will need to update the default image or create a new one. Attention CMU Lab Authors The default autograding image at CMU is called rhel.img and is a copy of the software on the CMU Andrew machines ( linux.andrew.cmu.edu ). If you need custom software installed, please send mail to autolab-help@andrew.cmu.edu. If you want a scoreboard, you should select \"Add Scoreboard,\" which will allow you to specify the number of columns and their names. The \"Add Scoreboard\" page contains a tutorial on how to do this. You'll also need to define the names and point values for all the problems in this lab, including the autograded ones. Each student submission is a single file, either a text source file or an archive file containing multiple files and directories. You'll need to specify the base name for the student submission files (e.g., mm.c , handin.tar ).","title":"Step 2: Configure the lab for autograding."},{"location":"lab/#step-3-add-the-required-autograding-files","text":"For an autograded lab, Autolab expects the following two autograding files in the lab directory: autograde-Makefile : runs the autograder on a student submission. autograde.tar : contains all of the files (except for the student handin file) that are needed for autograding. Each time a student submits their work or an instructor requests a regrade, Autolab copies the student handin file, along with the two autograding files, to an empty directory on an autograding instance , renames the student handin file to base name (e.g., hello.c, handin.tar), renames autograde-Makefile to Makefile , executes the command make on the autograding instance, and finally captures the stdout generated by the autograder, and parses the resulting JSON autoresult to determine the autograded scores.","title":"Step 3: Add the required autograding files."},{"location":"lab/#importing-an-autograded-lab-from-a-previous-semester","text":"If you've created a lab for a course in a previous semester and have access to the lab directory (as we do at CMU via AFS), you can import the lab into your current course by copying the lab directory from the previous course to the current course, cleaning out the handin directory, then visiting the \"Install Assessment\" page and selecting \"Option 2: Import an existing assessment from the file system.\" Autolab will give you a list of all of the directories that appear to be uninstalled labs, from which you can select your particular lab. If you don't have access to the lab directory, another option is to import a lab from a tarball that was created by running \"Export assessment\" in an instance of a lab from a previous semester. Visit the \"Install Assessment\" page and select \"Option 3: Import an existing assessment from tarball.\" This will upload the tarball, create a new lab directory by expanding the tarball, and then import the directory.","title":"Importing an Autograded Lab from a Previous Semester"},{"location":"lab/#example-hello-lab","text":"In this section we'll look at the simplest possible autograded lab we could imagine, called, appropriately enough, the Hello Lab (with tarball ), which is stored in a lab directory called hello in the Autolab github repo. While it's trivial, it illustrates all of the aspects of developing an autograded lab, and provides a simple example that you can use for sanity testing on your Autolab installation. In this lab, students are asked to write a version of the K&R \"hello, world\" program, called hello.c . The autograder simply checks that the submitted hello.c program compiles and runs with an exit status of zero. If so, the submission gets 100 points. Otherwise it gets 0 points.","title":"Example: Hello Lab"},{"location":"lab/#directory-structure","text":"Autolab expects to find the autograde-Makefile and autograde.tar files in the hello lab directory, but otherwise places no constraints on the contents and organization of this directory. However, based on our experience, we strongly recommend a directory structure with the following form: hello/README : # Basic files created by the lab author Makefile Builds the lab from src/ README autograde-Makefile Makefile that runs the autograder src/ Contains all src files and solutions test-autograder/ For testing autograder offline writeup/ Lab writeup that students view from Autolab # Files created by running make hello-handout/ The directory that is handed out to students, created using files from src/. hello-handout.tar Archive of hello-handout directory autograde.tar File that is copied to the autograding instance (along with autograde-Makefile and student handin file) # Files created and managed by Autolab handin/ All students handin files hello.rb Config file hello.yml Database properties that persist from semester to semester log.txt Log of autograded submissions The key idea with this directory structure is to place all code for the lab in the src directory, including the autograding code and any starter code handed out to students in the handout directory ( hello-handout.tar in this example). Keeping all hard state in the src directory helps limit inconsistencies. The main makefile creates hello-handout by copying files from src , and then tars it up: hello/Makefile : # # Makefile to manage the example Hello Lab # # Get the name of the lab directory LAB = $( notdir $( PWD )) all : handout handout - tarfile handout : # Rebuild the handout directory that students download ( rm -rf $( LAB ) -handout ; mkdir $( LAB ) -handout ) cp -p src/Makefile-handout $( LAB ) -handout/Makefile cp -p src/README-handout $( LAB ) -handout/README cp -p src/hello.c-handout $( LAB ) -handout/hello.c cp -p src/driver.sh $( LAB ) -handout handout-tarfile : handout # Build *-handout.tar and autograde.tar tar cvf $( LAB ) -handout.tar $( LAB ) -handout cp -p $( LAB ) -handout.tar autograde.tar clean : # Clean the entire lab directory tree. Note that you can run # \"make clean; make\" at any time while the lab is live with no # adverse effects. rm -f *~ *.tar ( cd src ; make clean ) ( cd test-autograder ; make clean ) rm -rf $( LAB ) -handout rm -f autograde.tar # # CAREFULL!!! This will delete all student records in the logfile and # in the handin directory. Don't run this once the lab has started. # Use it to clean the directory when you are starting a new version # of the lab from scratch, or when you are debugging the lab prior # to releasing it to the students. # cleanallfiles : # Reset the lab from scratch. make clean rm -f log.txt rm -rf handin/* Filenames are disambiguated by appending -handout , which is stripped when they are copied to the handout directory. For example, src/hello.c is the instructor's solution file, and src/hello.c-handout is the starter code that is given to the students in hello-handout/hello.c . And src/README is the README for the src directory and src/README-handout is the README that is handed out to students in hello-handout/README . To build the lab, type make clean; make . You can do this as often as you like while the lab is live with no adverse effects. However, be careful to never type make cleanallfiles while the lab is live; this should only be done before the lab goes live; never during or after.","title":"Directory Structure"},{"location":"lab/#source-directory","text":"The hello/src/ directory contains all of the code files for the Hello Lab, including the files that are handed out to students: hello/src/README : # Autograder and solution files Makefile Makefile and ... README ... README for this directory driver . sh * Autograder hello . c Solution hello . c file # Files that are handed out to students Makefile - handout Makefile and ... README - handout ... README handed out to students hello . c - handout Blank hello . c file handed out to students","title":"Source Directory"},{"location":"lab/#handout-directory","text":"The hello/hello-handout/ directory contains the files that the students will use to work on the lab. It contains no hard state, and is populated entirely with files from hello/src : hello/hello-handout/README : For this lab , you should write a tiny C program , called \"hello.c\" , that prints \"hello, world\" to stdout and then indicates success by exiting with a status of zero . To test your work: $ make clean ; make ; ./ hello To run the same autograder that Autolab will use when you submit: $ ./ driver . sh Files: README This file Makefile Compiles hello . c driver . sh Autolab autograder hello . c Empty C file that you will edit hello/hello-handout/Makefile contains the rules that compile the student source code: # Student makefile for the Hello Lab all : gcc hello.c -o hello clean : rm -rf *~ hello To compile and run their code, students type: $ make clean ; make $ ./hello","title":"Handout Directory"},{"location":"lab/#autograder","text":"The autograder for the Hello Lab is a trivially simple bash script called driver.sh that compiles and runs hello.c and verifies that it returns with an exit status of zero: hello/src/driver.sh #!/bin/bash # driver.sh - The simplest autograder we could think of. It checks # that students can write a C program that compiles, and then # executes with an exit status of zero. # Usage: ./driver.sh # Compile the code echo \"Compiling hello.c\" ( make clean ; make ) status = $? if [ ${ status } -ne 0 ] ; then echo \"Failure: Unable to compile hello.c (return status = ${ status } )\" echo \"{\\\"scores\\\": {\\\"Correctness\\\": 0}}\" exit fi # Run the code echo \"Running ./hello\" ./hello status = $? if [ ${ status } -eq 0 ] ; then echo \"Success: ./hello runs with an exit status of 0\" echo \"{\\\"scores\\\": {\\\"Correctness\\\": 100}}\" else echo \"Failure: ./hello fails or returns nonzero exit status of ${ status } \" echo \"{\\\"scores\\\": {\\\"Correctness\\\": 0}}\" fi exit For example: $ ./driver.sh # Compiling hello.c # rm -rf *~ hello # gcc hello.c -o hello # Running ./hello # Hello, world # Success: ./hello runs with an exit status of 0 # {\"scores\": {\"Correctness\": 100}} Notice that the autograder expects the hello lab on the Autolab front-end to have been defined with a problem called \"Correctness\", with a maximum value of 100 points. If you forget to define the problems listed in the JSON autoresult, scores will still be logged, but they won't be posted to the database.","title":"Autograder"},{"location":"lab/#required-autograding-files","text":"Autolab requires two autograding files called autograde.tar , which contains all of the code required by the autograder, and autograde-Makefile , which runs the autograder on the autograding image when each submission is graded. For the Hello Lab, autograde.tar is simply a copy of the hello-handout.tar file that is handed out to students. And here is the corresponding hello/autograde-makefile : all : tar xvf autograde.tar cp hello.c hello-handout ( cd hello-handout ; ./driver.sh ) clean : rm -rf *~ hello-handout The makefile expands autograde.tar into hello-handout , copies hello.c (the submission file) into hello-handout , changes directory to hello-handout , builds the autograder, and then runs it.","title":"Required Autograding Files"},{"location":"lab/#test-directory","text":"For our labs, we like to setup a test directory (called test-autograder in this example), that allows us to test our autograde-Makefile and autograde-tar files by simulating Autolab's behavior on the autograding instance. The test-autograder directory has the following form: $ cd test-autograder $ ls -l # total 3 # lrwxr-xr-x 1 droh users 21 Aug 4 16:43 Makefile -> autograde-Makefile # lrwxr-xr-x 1 droh users 16 Aug 4 16:43 autograde.tar -> autograde.tar # -rw-rw-r-- 1 droh users 113 Aug 4 16:44 hello.c To simulate Autolab's behavior on an autograding instance: $ cd test-autograder && make clean && make # Running ./hello # Hello, world # Success: ./hello runs with an exit status of 0 # {\"scores\": {\"Correctness\": 100}}","title":"Test Directory"},{"location":"lab/#writeup-directory","text":"The hello/writeup contains the detailed lab writeup, either html or pdf file, that students can download from the Autolab front end.","title":"Writeup directory"},{"location":"lab/#faq","text":"","title":"FAQ"},{"location":"lab/#why-is-autolab-not-displaying-my-stdout-output","text":"Autolab always shows the stdout output of running make, even when the program crashed or timed out. However, when it does crash and the expected autoresult json string is not appended to the output, parsing of the last line will fail. If this happens, any stdout output that is longer than 10,000 lines will be discarded (Note that this limit does not apply when the autoresult json is valid).","title":"Why is Autolab not displaying my stdout output?"},{"location":"lab/#is-there-a-way-to-formatting-the-feedback-provided-to-the-students","text":"Yes there is, by using the formatted feedback feature .","title":"Is there a way to formatting the feedback provided to the students"},{"location":"one-click/","text":"Autolab + Tango OneClick Installation Attention The OneClick installation is currently broken. We are in the midst of migrating to Docker for installation. Please treat this page as legacy reference. OneClick is the fastest way to install Autolab and Tango on an Ubuntu VM. The installation uses packages Autolab, MySQL, and Tango into seperate Docker containers with specific exposed ports for communication. There are two types of installations. A local development setup and a real-world ready setup that requires SSL certificates, email service configuration, and domain name registration. Use the local setup for experimentation before deploying in a real-world scenario on such apps like Heroku, EC2, or DigitalOcean, among others. Local OneClick Setup 1. Prepare an Ubuntu VM These installation instructions are for Ubuntu. If you're on other operating system, we recommend you set up an Ubuntu virtual machine first with Virtual Box . About the System Configuration: Ubuntu 14.04( or higher) 64bit 2GB memory + 20GB disk To set up, Install Ubuntu on Virtualbox may help you. Optional: For better experience, we also recommend you to \"insert guest additional CD image\" for your virtual machine to enable full screen. (If you installed Ubuntu 16+, you can skip this) Devices > Insert guest additional CD image Also enable clipboard share for easier copy and paste between host and VM. Settings > Advanced > Shared Clipboard > Bidrectional You need to restart your virtual machine to validate these optional changes. 2. Download Root is required to install Autolab: sudo -i Clone repo: git clone https://github.com/autolab/autolab-oneclick.git ; cd autolab-oneclick 3. Installation Run the following in the autolab-oneclick folder ./install.sh -l This will take a few minutes. Once you see Autolab Installation Finished , ensure all docker containers are running: docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES # c8679844bbfa local_web \"/sbin/my_init\" 3 months ago Exited (0) 3 months ago local_web_1 721 kB (virtual 821 MB) # 45a9e30241ea mysql \"docker-entrypoint...\" 3 months ago Exited (0) 3 months ago 0.0.0.0:32768->3306/tcp local_db_1 0 B (virtual 383 MB) # 1ef089e2dca4 local_tango \"sh start.sh\" 3 months ago Exited (0) 3 months ago 0.0.0.0:8600->8600/tcp local_tango_1 91.1 kB (virtual 743 MB) Now Autolab is successfully installed and running on your virtual machine. Open your browser and visit localhost:3000 , you will see the landing page of Autolab. Follow the instructions here to test out your set up. Server/Production OneClick Setup 1. Provision a Server Server If you don't already have a server, we recommend a VPS (virtual private server). Here are a couple popular VPS providers: DigitalOcean (recommended) Amazon Lightsail Google Cloud Platform Domain name (A domain name is both required by SSL and email service.) In your DNS provider: Add www and @ records pointing to the ip address of your server. Add DKIM and SFF records by creating TXT records after you finish the email service part. SSL You can run Autolab with or without HTTPS encryption. We strongly recommend you run it with HTTPS. Here are a few options to get the SSL certificate and key: Go through your school/organization Many universities have a program whereby they'll grant SSL certificates to students and faculty for free. Some of these programs require you to be using a school-related domain name, but some don't. You should be able to find out more information from your school's IT department. Use paid service: SSLmate You can follow this simple guide to get your paid SSL with SSLMate in the simplest way. Email Service Autolab uses email for various features, include sending out user confirmation emails and instructor-to-student bulk emails. You can use MailChimp + Mandrill to configure transactional email. Create a MailChimp account here Add Mandrill using these instructions Go to the settings page and create a new API key From the Mailchimp/Mandrill Domains settings page, add your domain Configure the DKIM and SFF settings by creating TXT records with your DNS provider (they link to some instructions for how to do this, but the process will differ depending on which DNS provider you are using. Try Google!). 2. Download and Configuration Use root to install Autolab sudo -i Clone the installation package git clone https://github.com/autolab/autolab-oneclick.git ; cd autolab-oneclick Generate a new secret key for Devise Auth Configuration: python -c \"import random; print hex(random.getrandbits(512))[2:-1]\" Update the values in server/configs/devise.rb config . secret_key = < GENERATED_SECRET_KEY > config . mailer_sender = < EMAIL_ADDRESS_WITH_YOUR_HOSTNAME > With SSL: Copy your SSL certificate and key file into the server/ssl directory. Without SSL : Comment out the following lines in server/configs/nginx.conf # EFF recommended SSL settings # ssl_prefer_server_ciphers on; # ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:ECDH+3DES:RSA+AES:RSA+3DES:!ADH:!AECDH:!MD5:!DSS; # ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; Comment out the following line in server/configs/production.rb # config.middleware.use Rack::SslEnforcer, :except => [ /log_submit/, /local_submit/ ] Configure Nginx in server/configs/nginx.conf server_name < YOUR_SERVER_DOMAIN > ssl_certificate /path/ to / ssl_certificate / file ssl_certificate_key /path/ to / ssl_certificate_key / file Configure Email in server/configs/production.rb . Update the address, port, user_name, password and domain with your email service informations. For Mandrill, go to \"SMTP & API Info\" to see the informations. 3. Installation Start Installation cd autolab-oneclick ./install.sh -s Answer the prompts and wait until you see Autolab Installation Finished . Ensure docker containers are running docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES # c8679844bbfa local_web \"/sbin/my_init\" 3 months ago Exited (0) 3 months ago local_web_1 721 kB (virtual 821 MB) # 45a9e30241ea mysql \"docker-entrypoint...\" 3 months ago Exited (0) 3 months ago 0.0.0.0:32768->3306/tcp local_db_1 0 B (virtual 383 MB) # 1ef089e2dca4 local_tango \"sh start.sh\" 3 months ago Exited (0) 3 months ago 0.0.0.0:8600->8600/tcp local_tango_1 91.1 kB (virtual 743 MB) Now Autolab is successfully installed and running on your virtual machine. Open your browser and visit https://yourdomainname , to see the landing page of Autolab. Follow the instructions here to test out your set up. Testing Login with the following credentials: email: admin@foo.bar password: adminfoobar We have populated dummy data for you to test with. Run the following commands to cleanup the dummy data: cd local docker-compose run --rm -e RAILS_ENV = production web rake autolab:depopulate","title":"OneClick Install"},{"location":"one-click/#autolab-tango-oneclick-installation","text":"Attention The OneClick installation is currently broken. We are in the midst of migrating to Docker for installation. Please treat this page as legacy reference. OneClick is the fastest way to install Autolab and Tango on an Ubuntu VM. The installation uses packages Autolab, MySQL, and Tango into seperate Docker containers with specific exposed ports for communication. There are two types of installations. A local development setup and a real-world ready setup that requires SSL certificates, email service configuration, and domain name registration. Use the local setup for experimentation before deploying in a real-world scenario on such apps like Heroku, EC2, or DigitalOcean, among others.","title":"Autolab + Tango OneClick Installation"},{"location":"one-click/#local-oneclick-setup","text":"","title":"Local OneClick Setup"},{"location":"one-click/#1-prepare-an-ubuntu-vm","text":"These installation instructions are for Ubuntu. If you're on other operating system, we recommend you set up an Ubuntu virtual machine first with Virtual Box . About the System Configuration: Ubuntu 14.04( or higher) 64bit 2GB memory + 20GB disk To set up, Install Ubuntu on Virtualbox may help you. Optional: For better experience, we also recommend you to \"insert guest additional CD image\" for your virtual machine to enable full screen. (If you installed Ubuntu 16+, you can skip this) Devices > Insert guest additional CD image Also enable clipboard share for easier copy and paste between host and VM. Settings > Advanced > Shared Clipboard > Bidrectional You need to restart your virtual machine to validate these optional changes.","title":"1. Prepare an Ubuntu VM"},{"location":"one-click/#2-download","text":"Root is required to install Autolab: sudo -i Clone repo: git clone https://github.com/autolab/autolab-oneclick.git ; cd autolab-oneclick","title":"2. Download"},{"location":"one-click/#3-installation","text":"Run the following in the autolab-oneclick folder ./install.sh -l This will take a few minutes. Once you see Autolab Installation Finished , ensure all docker containers are running: docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES # c8679844bbfa local_web \"/sbin/my_init\" 3 months ago Exited (0) 3 months ago local_web_1 721 kB (virtual 821 MB) # 45a9e30241ea mysql \"docker-entrypoint...\" 3 months ago Exited (0) 3 months ago 0.0.0.0:32768->3306/tcp local_db_1 0 B (virtual 383 MB) # 1ef089e2dca4 local_tango \"sh start.sh\" 3 months ago Exited (0) 3 months ago 0.0.0.0:8600->8600/tcp local_tango_1 91.1 kB (virtual 743 MB) Now Autolab is successfully installed and running on your virtual machine. Open your browser and visit localhost:3000 , you will see the landing page of Autolab. Follow the instructions here to test out your set up.","title":"3. Installation"},{"location":"one-click/#serverproduction-oneclick-setup","text":"","title":"Server/Production OneClick Setup"},{"location":"one-click/#1-provision-a-server","text":"Server If you don't already have a server, we recommend a VPS (virtual private server). Here are a couple popular VPS providers: DigitalOcean (recommended) Amazon Lightsail Google Cloud Platform Domain name (A domain name is both required by SSL and email service.) In your DNS provider: Add www and @ records pointing to the ip address of your server. Add DKIM and SFF records by creating TXT records after you finish the email service part. SSL You can run Autolab with or without HTTPS encryption. We strongly recommend you run it with HTTPS. Here are a few options to get the SSL certificate and key: Go through your school/organization Many universities have a program whereby they'll grant SSL certificates to students and faculty for free. Some of these programs require you to be using a school-related domain name, but some don't. You should be able to find out more information from your school's IT department. Use paid service: SSLmate You can follow this simple guide to get your paid SSL with SSLMate in the simplest way.","title":"1. Provision a Server"},{"location":"one-click/#email-service","text":"Autolab uses email for various features, include sending out user confirmation emails and instructor-to-student bulk emails. You can use MailChimp + Mandrill to configure transactional email. Create a MailChimp account here Add Mandrill using these instructions Go to the settings page and create a new API key From the Mailchimp/Mandrill Domains settings page, add your domain Configure the DKIM and SFF settings by creating TXT records with your DNS provider (they link to some instructions for how to do this, but the process will differ depending on which DNS provider you are using. Try Google!).","title":"Email Service"},{"location":"one-click/#2-download-and-configuration","text":"Use root to install Autolab sudo -i Clone the installation package git clone https://github.com/autolab/autolab-oneclick.git ; cd autolab-oneclick Generate a new secret key for Devise Auth Configuration: python -c \"import random; print hex(random.getrandbits(512))[2:-1]\" Update the values in server/configs/devise.rb config . secret_key = < GENERATED_SECRET_KEY > config . mailer_sender = < EMAIL_ADDRESS_WITH_YOUR_HOSTNAME > With SSL: Copy your SSL certificate and key file into the server/ssl directory. Without SSL : Comment out the following lines in server/configs/nginx.conf # EFF recommended SSL settings # ssl_prefer_server_ciphers on; # ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:ECDH+3DES:RSA+AES:RSA+3DES:!ADH:!AECDH:!MD5:!DSS; # ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; Comment out the following line in server/configs/production.rb # config.middleware.use Rack::SslEnforcer, :except => [ /log_submit/, /local_submit/ ] Configure Nginx in server/configs/nginx.conf server_name < YOUR_SERVER_DOMAIN > ssl_certificate /path/ to / ssl_certificate / file ssl_certificate_key /path/ to / ssl_certificate_key / file Configure Email in server/configs/production.rb . Update the address, port, user_name, password and domain with your email service informations. For Mandrill, go to \"SMTP & API Info\" to see the informations.","title":"2. Download and Configuration"},{"location":"one-click/#3-installation_1","text":"Start Installation cd autolab-oneclick ./install.sh -s Answer the prompts and wait until you see Autolab Installation Finished . Ensure docker containers are running docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES # c8679844bbfa local_web \"/sbin/my_init\" 3 months ago Exited (0) 3 months ago local_web_1 721 kB (virtual 821 MB) # 45a9e30241ea mysql \"docker-entrypoint...\" 3 months ago Exited (0) 3 months ago 0.0.0.0:32768->3306/tcp local_db_1 0 B (virtual 383 MB) # 1ef089e2dca4 local_tango \"sh start.sh\" 3 months ago Exited (0) 3 months ago 0.0.0.0:8600->8600/tcp local_tango_1 91.1 kB (virtual 743 MB) Now Autolab is successfully installed and running on your virtual machine. Open your browser and visit https://yourdomainname , to see the landing page of Autolab. Follow the instructions here to test out your set up.","title":"3. Installation"},{"location":"one-click/#testing","text":"Login with the following credentials: email: admin@foo.bar password: adminfoobar We have populated dummy data for you to test with. Run the following commands to cleanup the dummy data: cd local docker-compose run --rm -e RAILS_ENV = production web rake autolab:depopulate","title":"Testing"},{"location":"tango-cli/","text":"Tango Command Line Client This is a guide to use the command-line client ( clients/tango-cli.py ) to test and collect other valuable information from Tango. Please setup Tango before moving forward. This guide assumes an instance of Tango is already up and running. Running a Sample Job The CLI supports two ways to run a sample job, individual steps or in a single all-in-one command . The first option is better for debugging each individual API call, whereas the second option is best for quickly running a job. Other Tango CLI commands are also discussed below . The Tango directory contains various different jobs in the clients/ directory; clients/README.md discusses the function of each job. Find out more information about the Tango REST API here . Single Command The --runJob command simply runs a job from a directory of files by uploading all the files in the directory. You can use this to submit an autograding job by running $ python clients/tango-cli.py -P 3000 -k test -l assessment1 --runJob clients/job1/ --image autograding_image The args are -P <port>, -k <key>, -l <unique_job_name> --runJob <job_files_path> --image <autograde_image> Individual Steps Open a courselab on Tango. This will create a directory for tango to store the files for the job. $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> --open Upload files necessary for the job. $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --upload --filename <clients/job1/hello.sh> $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --upload --filename <clients/job1/autograde-Makefile> Add the job to the queue. Note: localFile is the name of the file that was uploaded and destFile is the name of the file that will be on the VM. One of the destFile attributes must be Makefile . Furthermore, image references the name of the VM image you want the job to be run on. For Docker it is autograding_image . $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --addJob --infiles \\ '{\"localFile\" : \"hello.sh\", \"destFile\" : \"hello.sh\"}' \\ '{\"localFile\" : \"autograde-Makefile\", \"destFile\" : \"Makefile\"}' \\ --image <image> --outputFile <outputFileName> \\ --jobname <jobname> --maxsize <maxOutputSize> --timeout <jobTimeout> Get the job output. $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --poll --outputFile <outputFileName> The output file will have the following header: Autograder [ <date-time> ] : Received job <jobname>:<jobid> Autograder [ <date-time> ] : Success: Autodriver returned normally Autograder [ <date-time> ] : Here is the output from the autograder: Miscellaneous Commands The CLI also implements a list of commands to invoke the Tango REST API , including --info , --prealloc , and --jobs . For a full list of commands, run: python clients/tango-cli.py --help The general form for each command is as follows: python clients/tango-cli.py -P <port> -k <key> <command>","title":"Tango CLI"},{"location":"tango-cli/#tango-command-line-client","text":"This is a guide to use the command-line client ( clients/tango-cli.py ) to test and collect other valuable information from Tango. Please setup Tango before moving forward. This guide assumes an instance of Tango is already up and running.","title":"Tango Command Line Client"},{"location":"tango-cli/#running-a-sample-job","text":"The CLI supports two ways to run a sample job, individual steps or in a single all-in-one command . The first option is better for debugging each individual API call, whereas the second option is best for quickly running a job. Other Tango CLI commands are also discussed below . The Tango directory contains various different jobs in the clients/ directory; clients/README.md discusses the function of each job. Find out more information about the Tango REST API here .","title":"Running a Sample Job"},{"location":"tango-cli/#single-command","text":"The --runJob command simply runs a job from a directory of files by uploading all the files in the directory. You can use this to submit an autograding job by running $ python clients/tango-cli.py -P 3000 -k test -l assessment1 --runJob clients/job1/ --image autograding_image The args are -P <port>, -k <key>, -l <unique_job_name> --runJob <job_files_path> --image <autograde_image>","title":"Single Command"},{"location":"tango-cli/#individual-steps","text":"Open a courselab on Tango. This will create a directory for tango to store the files for the job. $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> --open Upload files necessary for the job. $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --upload --filename <clients/job1/hello.sh> $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --upload --filename <clients/job1/autograde-Makefile> Add the job to the queue. Note: localFile is the name of the file that was uploaded and destFile is the name of the file that will be on the VM. One of the destFile attributes must be Makefile . Furthermore, image references the name of the VM image you want the job to be run on. For Docker it is autograding_image . $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --addJob --infiles \\ '{\"localFile\" : \"hello.sh\", \"destFile\" : \"hello.sh\"}' \\ '{\"localFile\" : \"autograde-Makefile\", \"destFile\" : \"Makefile\"}' \\ --image <image> --outputFile <outputFileName> \\ --jobname <jobname> --maxsize <maxOutputSize> --timeout <jobTimeout> Get the job output. $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --poll --outputFile <outputFileName> The output file will have the following header: Autograder [ <date-time> ] : Received job <jobname>:<jobid> Autograder [ <date-time> ] : Success: Autodriver returned normally Autograder [ <date-time> ] : Here is the output from the autograder:","title":"Individual Steps"},{"location":"tango-cli/#miscellaneous-commands","text":"The CLI also implements a list of commands to invoke the Tango REST API , including --info , --prealloc , and --jobs . For a full list of commands, run: python clients/tango-cli.py --help The general form for each command is as follows: python clients/tango-cli.py -P <port> -k <key> <command>","title":"Miscellaneous Commands"},{"location":"tango-deploy/","text":"Deploying Standalone Tango This is a guide to setup a fully self-sufficient Tango deployment environment out-of-the-box using Docker. The suggested deployment pattern for Tango uses Nginx as a proxy and Supervisor as a process manager for Tango and all its dependencies. All requests to Nginx are rerouted to a Tango process. Details Nginx default port - 8600 Tango ports - 8610, 8611 Redis port - 6379 You can change any of these in the respective config files in deployment/config/ before you build the tango_deployment image. Steps Clone the Tango repo $ git clone https://github.com/autolab/Tango.git ; cd Tango Create a config.py file from the given template. $ cp config.template.py config.py Modify DOCKER_VOLUME_PATH in config.py as follows: DOCKER_VOLUME_PATH = '/opt/TangoService/Tango/volumes/' Install docker on the host machine by following instructions on the docker installation page . Ensure docker is running: $ docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Run the following command to build the Tango deployment image. $ docker build --tag = \"tango_deployment\" . Ensure the image was built by running. $ docker images # REPOSITORY TAG IMAGE ID CREATED SIZE # tango_deployment latest 3c0d4f4b4958 2 minutes ago 742.6 MB # ubuntu 15.04 d1b55fd07600 4 minutes ago 131.3 MB Run the following command to access the image in a container with a bash shell. The -p flag will map nginxPort on the docker container to localPort (8610 recommended) on your local machine (or on the VM that docker is running in on the local machine) so that Tango is accessible from outside the docker container. $ docker run --privileged -p <localPort>:<nginxPort> -it tango_deployment /bin/bash Set up a VMMS for Tango within the Docker container. Docker ( recommended ) Amazon EC2 Run the following command to start supervisor, which will then start Tango and all its dependencies. $ service supervisor start Check to see if Tango is responding to requests $ curl localhost:8610 # Hello, world! RESTful Tango here! Once you have a VMMS set up, leave the tango_deployment container by typing exit and once back in the host shell run the following command to get the name of your production container. $ docker ps -as # CONTAINER ID IMAGE COMMAND NAMES SIZE # c704d45c3737 tango_deployment \"/bin/bash\" erwin 40.26 MB The container created in this example has the name ` erwin ` . The name of the production container can be changed by running the following command and will be used to run the container and create services. $ docker rename <old_name> <new_name> To reopen the container once it has been built use the following command. This will reopen the interactive shell within the container and allow for configuration of the container after its initial run. $ docker start erwin $ docker attach erwin Once the container is set up with the autograding image, and the VMMS configured with any necessary software/environments needed for autograding (java, perl, etc), some configurations need to be changed to make the container daemon ready. Using the CONTAINER ID above, use the following commands to modify that containers config.v2.json file. $ sudo ls /var/lib/docker/containers c704d45c37372a034cb97761d99f6f3f362707cc23d689734895e017eda3e55b $ sudo vim /var/lib/docker/containers/c704d45c37372a034cb97761d99f6f3f362707cc23d689734895e017eda3e55b/config.v2.json Edit the \"Path\" field in the config.v2.json file from \"/bin/bash\" to \"/usr/bin/supervisord\" and save the file. Run the following commands to verify the changes were successful. The COMMAND field should now be \"/usr/bin/supervisord\" $ service docker restart $ docker ps -as # CONTAINER ID IMAGE COMMAND NAMES SIZE # c704d45c3737 tango_deployment \"/usr/bin/supervisord\" erwin 40.26 MB At this point when the container is started, the environment is fully set up and will no longer be an interactive shell. Instead, it will be the supervisor service that starts Tango and all its dependencies. Test this with the following commands and ensure Tango is functioning properly. $ docker start erwin # (Test tango environment) $ docker stop erwin Test the setup by running sample jobs using the testing guide . The following steps are optional and should only be used if you would like the Tango container to start on system boot. To ensure Tango starts with the system in the production environment, the container needs to be configured as a service. Below is a sample service config file that needs to be changed to suit your environment and placed in /etc/systemd/system/ . The file should be named <name>.service . For this example, it is erwin.service . [ Unit ] Description = Docker Service Managing Tango Container Requires = docker.service After = docker.service [ Service ] Restart = always ExecStart = /usr/bin/docker start -a erwin ExecStop = /usr/bin/docker stop -t 2 erwin [ Install ] WantedBy = default.target Test and ensure the service was set up correctly. The service should start successfully and remain running. $ systemctl daemon-reload $ service erwin start $ service erwin status Enable the service at system startup and reboot and ensure it starts with the host. $ systemctl enable erwin.service $ sudo reboot # (Server Reboots) $ service erwin status","title":"Deploying Tango"},{"location":"tango-deploy/#deploying-standalone-tango","text":"This is a guide to setup a fully self-sufficient Tango deployment environment out-of-the-box using Docker. The suggested deployment pattern for Tango uses Nginx as a proxy and Supervisor as a process manager for Tango and all its dependencies. All requests to Nginx are rerouted to a Tango process.","title":"Deploying Standalone Tango"},{"location":"tango-deploy/#details","text":"Nginx default port - 8600 Tango ports - 8610, 8611 Redis port - 6379 You can change any of these in the respective config files in deployment/config/ before you build the tango_deployment image.","title":"Details"},{"location":"tango-deploy/#steps","text":"Clone the Tango repo $ git clone https://github.com/autolab/Tango.git ; cd Tango Create a config.py file from the given template. $ cp config.template.py config.py Modify DOCKER_VOLUME_PATH in config.py as follows: DOCKER_VOLUME_PATH = '/opt/TangoService/Tango/volumes/' Install docker on the host machine by following instructions on the docker installation page . Ensure docker is running: $ docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Run the following command to build the Tango deployment image. $ docker build --tag = \"tango_deployment\" . Ensure the image was built by running. $ docker images # REPOSITORY TAG IMAGE ID CREATED SIZE # tango_deployment latest 3c0d4f4b4958 2 minutes ago 742.6 MB # ubuntu 15.04 d1b55fd07600 4 minutes ago 131.3 MB Run the following command to access the image in a container with a bash shell. The -p flag will map nginxPort on the docker container to localPort (8610 recommended) on your local machine (or on the VM that docker is running in on the local machine) so that Tango is accessible from outside the docker container. $ docker run --privileged -p <localPort>:<nginxPort> -it tango_deployment /bin/bash Set up a VMMS for Tango within the Docker container. Docker ( recommended ) Amazon EC2 Run the following command to start supervisor, which will then start Tango and all its dependencies. $ service supervisor start Check to see if Tango is responding to requests $ curl localhost:8610 # Hello, world! RESTful Tango here! Once you have a VMMS set up, leave the tango_deployment container by typing exit and once back in the host shell run the following command to get the name of your production container. $ docker ps -as # CONTAINER ID IMAGE COMMAND NAMES SIZE # c704d45c3737 tango_deployment \"/bin/bash\" erwin 40.26 MB The container created in this example has the name ` erwin ` . The name of the production container can be changed by running the following command and will be used to run the container and create services. $ docker rename <old_name> <new_name> To reopen the container once it has been built use the following command. This will reopen the interactive shell within the container and allow for configuration of the container after its initial run. $ docker start erwin $ docker attach erwin Once the container is set up with the autograding image, and the VMMS configured with any necessary software/environments needed for autograding (java, perl, etc), some configurations need to be changed to make the container daemon ready. Using the CONTAINER ID above, use the following commands to modify that containers config.v2.json file. $ sudo ls /var/lib/docker/containers c704d45c37372a034cb97761d99f6f3f362707cc23d689734895e017eda3e55b $ sudo vim /var/lib/docker/containers/c704d45c37372a034cb97761d99f6f3f362707cc23d689734895e017eda3e55b/config.v2.json Edit the \"Path\" field in the config.v2.json file from \"/bin/bash\" to \"/usr/bin/supervisord\" and save the file. Run the following commands to verify the changes were successful. The COMMAND field should now be \"/usr/bin/supervisord\" $ service docker restart $ docker ps -as # CONTAINER ID IMAGE COMMAND NAMES SIZE # c704d45c3737 tango_deployment \"/usr/bin/supervisord\" erwin 40.26 MB At this point when the container is started, the environment is fully set up and will no longer be an interactive shell. Instead, it will be the supervisor service that starts Tango and all its dependencies. Test this with the following commands and ensure Tango is functioning properly. $ docker start erwin # (Test tango environment) $ docker stop erwin Test the setup by running sample jobs using the testing guide . The following steps are optional and should only be used if you would like the Tango container to start on system boot. To ensure Tango starts with the system in the production environment, the container needs to be configured as a service. Below is a sample service config file that needs to be changed to suit your environment and placed in /etc/systemd/system/ . The file should be named <name>.service . For this example, it is erwin.service . [ Unit ] Description = Docker Service Managing Tango Container Requires = docker.service After = docker.service [ Service ] Restart = always ExecStart = /usr/bin/docker start -a erwin ExecStop = /usr/bin/docker stop -t 2 erwin [ Install ] WantedBy = default.target Test and ensure the service was set up correctly. The service should start successfully and remain running. $ systemctl daemon-reload $ service erwin start $ service erwin status Enable the service at system startup and reboot and ensure it starts with the host. $ systemctl enable erwin.service $ sudo reboot # (Server Reboots) $ service erwin status","title":"Steps"},{"location":"tango-rest/","text":"This page documents the REST API for submitting jobs to Tango. Authentication In order to have access to the REST interface of the Tango server, clients will first have to obtain a key from the Tango server. This key is a unique identifier of the client and it must be supplied with every HTTP request to the Tango server. If the Tango server fails to recognize the key, it does not entertain the request and returns an error message as part of the response body. Job Requests Here is a description of the requests that clients use to submit jobs: open A request to open consists of the client's key and an identifier for every lab, which is likely to be a combination of the course name and the lab name (i.e. courselab for autograding jobs). open checks if a directory for courselab exists. If a directory for courselab exists, a dict of MD5 hashes corresponding to every file in that directory is returned. If the directory does not exist, it is created and a folder for output files is also created within the courselab directory. Since no files exist in the newly created directory, an empty dict of MD5 hashes is returned. Request header: GET /open/key/courselab/ Request body: empty Response body: { \"statusMsg\" : <string> , \"statusId\" : <int> , \"files\" : { <fileName1> : <md5hash1>, <fileName2> : <md5hash2> ... }, } upload After receiving a list of MD5 hashes of files that exist on the Tango server, the client can choose to upload files that are different from the ones on the Tango server via successive upload commands. For each upload, the client must supply a filename header that gives the name of the file (on the local machine) to be uploaded to Tango. One of these files must be a Makefile, which needs to contain a rule called autograde (command to drive the autograding process). Request header: POST /upload/key/courselab/ Request body: <file> Response body: { \"statusMsg\" : <string> , \"statusId\" : <int> } addJob After uploading the appropriate files, the client uses this command to run the job for the files specified as files in the courselab and on an instance of a particular VM image . Each file has localFile and destFile attributes which specify what the file is called on the Tango server and what it should be called when copied over to a VM (for autograding) respectively. Exactly one of the specified files should have the destFile attribute set to Makefile , and the Makefile must contain a rule called autograde . Clients can also specify an optional timeout value ( timeout ) and maximum output file size ( max_kb ). This command is non-blocking and returns immediately with a status message. Additionally, the command accepts an optional parameter, callback_url . If the callback_url is specified, then the Tango server sends a POST request to the callback_url with the output file once the job is terminated. If the callback_url is not specified, the client can then send a poll request for the output_file to check the status of that job and retrieve the output file from the Tango server if autograding is complete. Request header: POST /addJob/key/courselab/ Request body: { \"image\" : <string> , # required VM image (e.g. \"rhel.img\" ) \"files\" : [ { \"localFile\" : <string> , \"destFile\" : <string> }, ... ], # required list of files to be used for autograding \"jobName\" : <string> , # required name of job \"output_file\" : <string> , # required name of output file \"timeout\" : <int> , # optional timeout value (secs) \"max_kb\" : <int> , # optional max output file size (KB) \"callback_url\" : <string> # optional URL for POST callback from server to client } Response body: { \"statusMsg\" : <string> , \"statusId\" : <int> , \"jobId\" : <int> } poll Check if the job for outputFile has completed. If not, return 404: Not Found and a status message, otherwise return the file in the response body, and free all resources held by the job. Request header: GET /poll/key/courselab/outputFile/ Request body: { <empty> } Response body: <autograder output file> if autograding successful otherwise: { \"statusMsg\" : <string> , \"statusId\" : <int> } Administrative Requests Here are the requests that administrators use to manage the Tango service, typically from a command line client. /info This is the \"hello, world\" request for the service. It returns a JSON object with some basic stats about the service, such as uptime, number of jobs, etc. Request header: GET /info/<KEY>/<COURSE_LAB>/ Request body: { <empty> } Response body: { \"info\" : { \"num_threads\" : <int> , \"job_requests\" : <int> , \"waitvm_timeouts\" : <int> , \"runjob_timeouts\" : <int> , \"elapsed_secs\" : <float> , \"runjob_errors\" : <int> , \"job_retries\" : <int> , \"copyin_errors\" : <int> , \"copyout_errors\" : <int> }, \"statusMsg\" : \"Found info successfully\" , \"statusId\" : 0 } /jobs Return a list of jobs. If deadjobs is set to 1, then return a list of recently completed jobs. Otherwise, return the list of currently running jobs. Note: This isn't strictly an admin request, since clients might find it useful to display jobs status, as we do in the Autolab front end. Request header: POST autograde.me/jobs/key/deadjobs/ Request body: empty Response body: JSON jobs object pool Returns a JSON object that provides info about the current state of a pool of instances spawned from some image . The response gives the total number of instances in the pool, and the number of free instances not currently allocated to any job. Request header: GET /pool/key/image/ Response body: JSON pool object prealloc Creates a pool of num identical instances spawned from image (e.g. \"rhel.img). Request header: POST /prealloc/key/image/num/ Request body: { \"vmms\" : <string> , # vmms to use (e.g. \"localSSH\" ) \"cores\" : <int> , # number of cores per VM \"memory\" : <int> , # amount of memory per VM } Response body: { \"status\": <string> } Implementation Notes Tango will maintain a directory for each of the labs in a course, which is created by open . All output files are stored within a specified output folder in this directory. Besides the runtime job queue, no other state is necessary. At job execution time, Tango will copy files specified by the files parameter in addJob to the VM. When the VM finishes, it will copy the output file back to the lab directory.","title":"REST API"},{"location":"tango-rest/#authentication","text":"In order to have access to the REST interface of the Tango server, clients will first have to obtain a key from the Tango server. This key is a unique identifier of the client and it must be supplied with every HTTP request to the Tango server. If the Tango server fails to recognize the key, it does not entertain the request and returns an error message as part of the response body.","title":"Authentication"},{"location":"tango-rest/#job-requests","text":"Here is a description of the requests that clients use to submit jobs:","title":"Job Requests"},{"location":"tango-rest/#open","text":"A request to open consists of the client's key and an identifier for every lab, which is likely to be a combination of the course name and the lab name (i.e. courselab for autograding jobs). open checks if a directory for courselab exists. If a directory for courselab exists, a dict of MD5 hashes corresponding to every file in that directory is returned. If the directory does not exist, it is created and a folder for output files is also created within the courselab directory. Since no files exist in the newly created directory, an empty dict of MD5 hashes is returned. Request header: GET /open/key/courselab/ Request body: empty Response body: { \"statusMsg\" : <string> , \"statusId\" : <int> , \"files\" : { <fileName1> : <md5hash1>, <fileName2> : <md5hash2> ... }, }","title":"open"},{"location":"tango-rest/#upload","text":"After receiving a list of MD5 hashes of files that exist on the Tango server, the client can choose to upload files that are different from the ones on the Tango server via successive upload commands. For each upload, the client must supply a filename header that gives the name of the file (on the local machine) to be uploaded to Tango. One of these files must be a Makefile, which needs to contain a rule called autograde (command to drive the autograding process). Request header: POST /upload/key/courselab/ Request body: <file> Response body: { \"statusMsg\" : <string> , \"statusId\" : <int> }","title":"upload"},{"location":"tango-rest/#addjob","text":"After uploading the appropriate files, the client uses this command to run the job for the files specified as files in the courselab and on an instance of a particular VM image . Each file has localFile and destFile attributes which specify what the file is called on the Tango server and what it should be called when copied over to a VM (for autograding) respectively. Exactly one of the specified files should have the destFile attribute set to Makefile , and the Makefile must contain a rule called autograde . Clients can also specify an optional timeout value ( timeout ) and maximum output file size ( max_kb ). This command is non-blocking and returns immediately with a status message. Additionally, the command accepts an optional parameter, callback_url . If the callback_url is specified, then the Tango server sends a POST request to the callback_url with the output file once the job is terminated. If the callback_url is not specified, the client can then send a poll request for the output_file to check the status of that job and retrieve the output file from the Tango server if autograding is complete. Request header: POST /addJob/key/courselab/ Request body: { \"image\" : <string> , # required VM image (e.g. \"rhel.img\" ) \"files\" : [ { \"localFile\" : <string> , \"destFile\" : <string> }, ... ], # required list of files to be used for autograding \"jobName\" : <string> , # required name of job \"output_file\" : <string> , # required name of output file \"timeout\" : <int> , # optional timeout value (secs) \"max_kb\" : <int> , # optional max output file size (KB) \"callback_url\" : <string> # optional URL for POST callback from server to client } Response body: { \"statusMsg\" : <string> , \"statusId\" : <int> , \"jobId\" : <int> }","title":"addJob"},{"location":"tango-rest/#poll","text":"Check if the job for outputFile has completed. If not, return 404: Not Found and a status message, otherwise return the file in the response body, and free all resources held by the job. Request header: GET /poll/key/courselab/outputFile/ Request body: { <empty> } Response body: <autograder output file> if autograding successful otherwise: { \"statusMsg\" : <string> , \"statusId\" : <int> }","title":"poll"},{"location":"tango-rest/#administrative-requests","text":"Here are the requests that administrators use to manage the Tango service, typically from a command line client.","title":"Administrative Requests"},{"location":"tango-rest/#info","text":"This is the \"hello, world\" request for the service. It returns a JSON object with some basic stats about the service, such as uptime, number of jobs, etc. Request header: GET /info/<KEY>/<COURSE_LAB>/ Request body: { <empty> } Response body: { \"info\" : { \"num_threads\" : <int> , \"job_requests\" : <int> , \"waitvm_timeouts\" : <int> , \"runjob_timeouts\" : <int> , \"elapsed_secs\" : <float> , \"runjob_errors\" : <int> , \"job_retries\" : <int> , \"copyin_errors\" : <int> , \"copyout_errors\" : <int> }, \"statusMsg\" : \"Found info successfully\" , \"statusId\" : 0 }","title":"/info"},{"location":"tango-rest/#jobs","text":"Return a list of jobs. If deadjobs is set to 1, then return a list of recently completed jobs. Otherwise, return the list of currently running jobs. Note: This isn't strictly an admin request, since clients might find it useful to display jobs status, as we do in the Autolab front end. Request header: POST autograde.me/jobs/key/deadjobs/ Request body: empty Response body: JSON jobs object","title":"/jobs"},{"location":"tango-rest/#pool","text":"Returns a JSON object that provides info about the current state of a pool of instances spawned from some image . The response gives the total number of instances in the pool, and the number of free instances not currently allocated to any job. Request header: GET /pool/key/image/ Response body: JSON pool object","title":"pool"},{"location":"tango-rest/#prealloc","text":"Creates a pool of num identical instances spawned from image (e.g. \"rhel.img). Request header: POST /prealloc/key/image/num/ Request body: { \"vmms\" : <string> , # vmms to use (e.g. \"localSSH\" ) \"cores\" : <int> , # number of cores per VM \"memory\" : <int> , # amount of memory per VM } Response body: { \"status\": <string> }","title":"prealloc"},{"location":"tango-rest/#implementation-notes","text":"Tango will maintain a directory for each of the labs in a course, which is created by open . All output files are stored within a specified output folder in this directory. Besides the runtime job queue, no other state is necessary. At job execution time, Tango will copy files specified by the files parameter in addJob to the VM. When the VM finishes, it will copy the output file back to the lab directory.","title":"Implementation Notes"},{"location":"tango-vmms/","text":"This page documents the interface for Tango's Virtual Machine Management Systems' (VMMSs) API and instructions for setting up VMMSs. See the vmms directory in Tango for example implementations. API The functions necessary to implement the API are documented here. Note that for certain implementations, some of these methods will be no-ops since the VMMS doesn't require any particular instructions to perform the specified actions. Furthermore, throughout this document, we use the term \"VM\" liberally to represent any container-like object on which Tango jobs may be run. initializeVM initializeVM ( self , vm ) Creates a new VM instance for the VMMS based on the fields of vm , which is a TangoMachine object defined in tangoObjects.py . waitVM waitVM ( self , vm , max_secs ) Waits at most max_secs for a VM to be ready to run jobs. Returns an error if the VM is not ready after max_secs . copyIn copyIn ( self , vm , inputFiles ) Copies the input files for a job into the VM. inputFiles is a list of InputFile objects defined in tangoObjects.py . For each InputFile object, file.localFile is the name of the file on the Tango host machine and file.destFile is what the name of the file should be on the VM. runJob runJob ( self , vm , runTimeout , maxOutputFileSize ) Runs the autodriver binary on the VM. The autodriver runs make on the VM (which in turn runs the job via the Makefile that was provided as a part of the input files for the job). The output from the autodriver most likely should be redirected to some feedback file to be used in the next method of the API. copyOut copyOut ( self , vm , destFile ) Copies the output file for the job out of the VM into destFile on the Tango host machine. destroyVM destroyVM ( self , vm ) Removes a VM from the Tango system. safeDestroyVM safeDestroyVM ( self , vm ) Removes a VM from the Tango system and makes sure that it has been removed. getVMs getVMs ( self ) Returns a complete list of VMs associated with this Tango system. Docker VMMS Setup This is a guide to set up Tango to run jobs inside Docker containers. Install docker on host machine by following instructions on the docker installation page . Ensure docker is running: $ docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Build base Docker image from root Tango directory. cd path/to/Tango docker build -t autograding_image vmms/ docker images autograding_image # Check if image built Update VMMS_NAME in config.py . # in config.py VMMS_NAME = \"localDocker\" Amazon EC2 VMMS Setup This is a guide to set up Tango to run jobs on an Amazon EC2 VM. Create an AWS Account or use an existing one. Obtain your access_key_id and secret_access_key by following the instructions here . Add AWS Credentials to a file called ~/.boto using the following format: [ Credentials ] aws_access_key_id = MYAMAZONTESTKEY12345 aws_secret_access_key = myawssecretaccesskey12345 Tango uses the Boto Python package to interface with Amazon Web Services In the AWS EC2 console, create an Ubuntu 14.04+ EC2 instance and save the .pem file in a safe location. Copy the directory and contents of autodriver/ in the Tango repo into the EC2 VM. For more help connecting to the EC2 instance follow this guide chmod 400 /path/my-key-pair.pem scp -i /path/my-key-pair.pem -r autodriver/ ubuntu@<ec2-host-name>.compute-1.amazonaws.com:~/ The autodriver is used as a sandbox environment to run the job inside the VM. It limits Disk I/O, Disk Usage, monitors security, and controls other valuable sudo level resources. In the EC2 VM, compile the autodriver. $ cd autodriver/ $ make clean ; make $ cp -p autodriver /usr/bin/autodriver Create the autograde Linux user and directory. All jobs will be run under this user. $ useradd autograde $ mkdir autograde $ chown autograde autograde $ chown :autograde autograde In the AWS EC2 console, create an AMI image from your EC2 VM. Use this guide to create a custom AMI. Exit the EC2 instance and edit the following values in config.py in the Tango directory. # VMMS to use. Must be set to a VMMS implemented in vmms/ before # starting Tango. Options are: \"localDocker\", \"distDocker\", # \"tashiSSH\", and \"ec2SSH\" VMMS_NAME = \"ec2SSH\" ###### # Part 5: EC2 Constants # EC2_REGION = 'us-east-1' # EC2 Region EC2_USER_NAME = 'ubuntu' # EC2 username DEFAULT_AMI = 'ami-4c99c35b' # Custom AMI Id DEFAULT_INST_TYPE = 't2.micro' # Instance Type DEFAULT_SECURITY_GROUP = 'autolab-autograde-ec2' # Security Group with full access to EC2 SECURITY_KEY_PATH = '/path/to/my-key-pair.pem' # Absolute path to my-key-pair.pem DYNAMIC_SECURITY_KEY_PATH = '' # Leave blank SECURITY_KEY_NAME = 'my-key-pair' # Name of the key file. Ex: if file name is 'my-key-pair.pem', fill value with 'my-key-pair' TANGO_RESERVATION_ID = '1' # Leave as 1 INSTANCE_RUNNING = 16 # Status code of a running instance, leave as 16 You should now be ready to run Tango jobs on EC2! Use the Tango CLI to test your setup.","title":"VMMS Docs"},{"location":"tango-vmms/#api","text":"The functions necessary to implement the API are documented here. Note that for certain implementations, some of these methods will be no-ops since the VMMS doesn't require any particular instructions to perform the specified actions. Furthermore, throughout this document, we use the term \"VM\" liberally to represent any container-like object on which Tango jobs may be run.","title":"API"},{"location":"tango-vmms/#initializevm","text":"initializeVM ( self , vm ) Creates a new VM instance for the VMMS based on the fields of vm , which is a TangoMachine object defined in tangoObjects.py .","title":"initializeVM"},{"location":"tango-vmms/#waitvm","text":"waitVM ( self , vm , max_secs ) Waits at most max_secs for a VM to be ready to run jobs. Returns an error if the VM is not ready after max_secs .","title":"waitVM"},{"location":"tango-vmms/#copyin","text":"copyIn ( self , vm , inputFiles ) Copies the input files for a job into the VM. inputFiles is a list of InputFile objects defined in tangoObjects.py . For each InputFile object, file.localFile is the name of the file on the Tango host machine and file.destFile is what the name of the file should be on the VM.","title":"copyIn"},{"location":"tango-vmms/#runjob","text":"runJob ( self , vm , runTimeout , maxOutputFileSize ) Runs the autodriver binary on the VM. The autodriver runs make on the VM (which in turn runs the job via the Makefile that was provided as a part of the input files for the job). The output from the autodriver most likely should be redirected to some feedback file to be used in the next method of the API.","title":"runJob"},{"location":"tango-vmms/#copyout","text":"copyOut ( self , vm , destFile ) Copies the output file for the job out of the VM into destFile on the Tango host machine.","title":"copyOut"},{"location":"tango-vmms/#destroyvm","text":"destroyVM ( self , vm ) Removes a VM from the Tango system.","title":"destroyVM"},{"location":"tango-vmms/#safedestroyvm","text":"safeDestroyVM ( self , vm ) Removes a VM from the Tango system and makes sure that it has been removed.","title":"safeDestroyVM"},{"location":"tango-vmms/#getvms","text":"getVMs ( self ) Returns a complete list of VMs associated with this Tango system.","title":"getVMs"},{"location":"tango-vmms/#docker-vmms-setup","text":"This is a guide to set up Tango to run jobs inside Docker containers. Install docker on host machine by following instructions on the docker installation page . Ensure docker is running: $ docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Build base Docker image from root Tango directory. cd path/to/Tango docker build -t autograding_image vmms/ docker images autograding_image # Check if image built Update VMMS_NAME in config.py . # in config.py VMMS_NAME = \"localDocker\"","title":"Docker VMMS Setup"},{"location":"tango-vmms/#amazon-ec2-vmms-setup","text":"This is a guide to set up Tango to run jobs on an Amazon EC2 VM. Create an AWS Account or use an existing one. Obtain your access_key_id and secret_access_key by following the instructions here . Add AWS Credentials to a file called ~/.boto using the following format: [ Credentials ] aws_access_key_id = MYAMAZONTESTKEY12345 aws_secret_access_key = myawssecretaccesskey12345 Tango uses the Boto Python package to interface with Amazon Web Services In the AWS EC2 console, create an Ubuntu 14.04+ EC2 instance and save the .pem file in a safe location. Copy the directory and contents of autodriver/ in the Tango repo into the EC2 VM. For more help connecting to the EC2 instance follow this guide chmod 400 /path/my-key-pair.pem scp -i /path/my-key-pair.pem -r autodriver/ ubuntu@<ec2-host-name>.compute-1.amazonaws.com:~/ The autodriver is used as a sandbox environment to run the job inside the VM. It limits Disk I/O, Disk Usage, monitors security, and controls other valuable sudo level resources. In the EC2 VM, compile the autodriver. $ cd autodriver/ $ make clean ; make $ cp -p autodriver /usr/bin/autodriver Create the autograde Linux user and directory. All jobs will be run under this user. $ useradd autograde $ mkdir autograde $ chown autograde autograde $ chown :autograde autograde In the AWS EC2 console, create an AMI image from your EC2 VM. Use this guide to create a custom AMI. Exit the EC2 instance and edit the following values in config.py in the Tango directory. # VMMS to use. Must be set to a VMMS implemented in vmms/ before # starting Tango. Options are: \"localDocker\", \"distDocker\", # \"tashiSSH\", and \"ec2SSH\" VMMS_NAME = \"ec2SSH\" ###### # Part 5: EC2 Constants # EC2_REGION = 'us-east-1' # EC2 Region EC2_USER_NAME = 'ubuntu' # EC2 username DEFAULT_AMI = 'ami-4c99c35b' # Custom AMI Id DEFAULT_INST_TYPE = 't2.micro' # Instance Type DEFAULT_SECURITY_GROUP = 'autolab-autograde-ec2' # Security Group with full access to EC2 SECURITY_KEY_PATH = '/path/to/my-key-pair.pem' # Absolute path to my-key-pair.pem DYNAMIC_SECURITY_KEY_PATH = '' # Leave blank SECURITY_KEY_NAME = 'my-key-pair' # Name of the key file. Ex: if file name is 'my-key-pair.pem', fill value with 'my-key-pair' TANGO_RESERVATION_ID = '1' # Leave as 1 INSTANCE_RUNNING = 16 # Status code of a running instance, leave as 16 You should now be ready to run Tango jobs on EC2! Use the Tango CLI to test your setup.","title":"Amazon EC2 VMMS Setup"},{"location":"tango/","text":"Tango Tango is a standalone RESTful Web service that runs jobs in virtual machines or containers. It was developed as a distributed grading system for Autolab and has been extensively used for autograding programming assignments. It is also open source and hosted on Github . Getting Started A brief overview of the Tango respository: tango.py - Main tango server jobQueue.py - Manages the job queue jobManager.py - Assigns jobs to free VMs worker.py - Shepherds a job through its execution preallocator.py - Manages pools of VMs vmms/ - VMMS library implementations restful-tango/ - HTTP server layer on the main Tango Tango runs jobs in VMs using a high level Virtual Memory Management System (VMMS) API. Tango currently has support for running jobs in Docker containers ( recommended ), Tashi VMs , or Amazon EC2 . For more information about the different Tango components, go to the following pages: REST API docs VMMS API docs Tango Architecture Overview Deploying Tango Installation This guide shows how to setup Tango in a development environment . Use the deploying Tango guide for installing in a production environment . Obtain the source code. git clone https://github.com/autolab/Tango.git ; cd Tango Install Redis following this guide . By default, Tango uses Redis as a stateless job queue. Learn more here . Create a config.py file from the given template. cp config.template.py config.py Create the course labs directory where job's output files will go, organized by key and lab name: mkdir courselabs By default the COURSELABS option in config.py points to the courselabs directory in the Tango directory. Change this to specify another path if you wish. Set up a VMMS for Tango to use. Docker ( recommended ) Amazon EC2 TashiVMMS (deprecated) Run the following commands to setup the Tango dev environment inside the Tango directory. Install pip if needed. $ pip install virtualenv $ virtualenv . $ source bin/activate $ pip install -r requirements.txt $ mkdir volumes If you are using Docker, set DOCKER_VOLUME_PATH in config.py to be the path to the volumes directory you just created. DOCKER_VOLUME_PATH = \"/path/to/Tango/volumes/\" Start Redis by running the following command: $ redis-server Run the following command to start the server (producer). If no port is given, the server will run on the port specified in config.py (default: 3000): python restful-tango/server.py <port> Open another terminal window and start the job manager (consumer): python jobManager.py For more information on the job producer/consumer model check out our blog post Ensure Tango is running: $ curl localhost:<port> # Hello, world! RESTful Tango here! You can test the Tango setup using the command line client . If you are using Tango with Autolab, you have to configure Autolab to use Tango. Go to your Autolab directory and enter the following commands: cp config/autogradeConfig.rb.template config/autogradeConfig.rb Fill in the correct info for your Tango deployment, mainly the following: # Hostname for Tango RESTful API RESTFUL_HOST = \"foo.bar.edu\" #(if you are running Tango locally, then it is just \"localhost\") # Port for Tango RESTful API RESTFUL_PORT = \"3000\" # Key for Tango RESTful API RESTFUL_KEY = \"test\" To deploy Tango in a standalone production environment, use this guide","title":"Getting Started"},{"location":"tango/#tango","text":"Tango is a standalone RESTful Web service that runs jobs in virtual machines or containers. It was developed as a distributed grading system for Autolab and has been extensively used for autograding programming assignments. It is also open source and hosted on Github .","title":"Tango"},{"location":"tango/#getting-started","text":"A brief overview of the Tango respository: tango.py - Main tango server jobQueue.py - Manages the job queue jobManager.py - Assigns jobs to free VMs worker.py - Shepherds a job through its execution preallocator.py - Manages pools of VMs vmms/ - VMMS library implementations restful-tango/ - HTTP server layer on the main Tango Tango runs jobs in VMs using a high level Virtual Memory Management System (VMMS) API. Tango currently has support for running jobs in Docker containers ( recommended ), Tashi VMs , or Amazon EC2 . For more information about the different Tango components, go to the following pages: REST API docs VMMS API docs Tango Architecture Overview Deploying Tango","title":"Getting Started"},{"location":"tango/#installation","text":"This guide shows how to setup Tango in a development environment . Use the deploying Tango guide for installing in a production environment . Obtain the source code. git clone https://github.com/autolab/Tango.git ; cd Tango Install Redis following this guide . By default, Tango uses Redis as a stateless job queue. Learn more here . Create a config.py file from the given template. cp config.template.py config.py Create the course labs directory where job's output files will go, organized by key and lab name: mkdir courselabs By default the COURSELABS option in config.py points to the courselabs directory in the Tango directory. Change this to specify another path if you wish. Set up a VMMS for Tango to use. Docker ( recommended ) Amazon EC2 TashiVMMS (deprecated) Run the following commands to setup the Tango dev environment inside the Tango directory. Install pip if needed. $ pip install virtualenv $ virtualenv . $ source bin/activate $ pip install -r requirements.txt $ mkdir volumes If you are using Docker, set DOCKER_VOLUME_PATH in config.py to be the path to the volumes directory you just created. DOCKER_VOLUME_PATH = \"/path/to/Tango/volumes/\" Start Redis by running the following command: $ redis-server Run the following command to start the server (producer). If no port is given, the server will run on the port specified in config.py (default: 3000): python restful-tango/server.py <port> Open another terminal window and start the job manager (consumer): python jobManager.py For more information on the job producer/consumer model check out our blog post Ensure Tango is running: $ curl localhost:<port> # Hello, world! RESTful Tango here! You can test the Tango setup using the command line client . If you are using Tango with Autolab, you have to configure Autolab to use Tango. Go to your Autolab directory and enter the following commands: cp config/autogradeConfig.rb.template config/autogradeConfig.rb Fill in the correct info for your Tango deployment, mainly the following: # Hostname for Tango RESTful API RESTFUL_HOST = \"foo.bar.edu\" #(if you are running Tango locally, then it is just \"localhost\") # Port for Tango RESTful API RESTFUL_PORT = \"3000\" # Key for Tango RESTful API RESTFUL_KEY = \"test\" To deploy Tango in a standalone production environment, use this guide","title":"Installation"}]}